{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "from FDIC import wsio, Logger\n",
    "import FDIC.constants as paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/kris/data_sources/fdic/BulkReports/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in glob.glob(path + '*[!.]'):\n",
    "    print(f'{folder}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = glob.glob(path + '*[!.]')\n",
    "print(f'{folders}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "df = pd.DataFrame({'Date': '20240506', 'As_of': datetime.datetime.now()}, index=[0])\n",
    "df0 = pd.DataFrame(columns=['Date', 'As_of'])\n",
    "df1 = pd.DataFrame({'Date': '20240506', 'As_of': datetime.datetime.now()}, index=[0])\n",
    "\n",
    "#df = pd.concat([df, df1], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = pd.concat([df0, df1], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(df1, df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(df['Date'])\n",
    "#df['Date'].isin(['20240506'])\n",
    "\n",
    "'20240506' in df['Date'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipped_dtypes = {'Date': str, 'As_of': datetime.datetime}\n",
    "skipped_dtypes.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "t = time.time()\n",
    "period = 60\n",
    "\n",
    "diff = t - period\n",
    "\n",
    "current_time = time.strftime('%H:%M:%S', time.localtime(t))\n",
    "wait_time = time.strftime('%H:%M:%S', time.localtime(diff))\n",
    "\n",
    "milliseconds_current = int((t - int(t)) * 1000)\n",
    "milliseconds_diff = int((diff - int(diff)) * 1000)\n",
    "\n",
    "print(f'current: {current_time}.{milliseconds_current}\\nwait: {wait_time}.{milliseconds_diff}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "current = time.time()\n",
    "#print(datetime.datetime.fromtimestamp(current + 3600).strftime(\"%Y%m%d %H:%M:%S\"))\n",
    "print(f'{time.strftime(\"%Y%m%d %H:%M:%S\",time.gmtime(current + 3600))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to your XBRL file and schema file\n",
    "\n",
    "bulkReport_path = '/Users/kris/data_sources/fdic/BulkReports/'\n",
    "schema_base_path = '/Users/kris/Desktop/BankRegScraper/taxonomy/Call_ 09302023_Form051'\n",
    "#schema_base_path = '/Users/kris/Downloads/_ 09302023_Form051/'\n",
    "\n",
    "xbrl_file_path = bulkReport_path + 'ABINGTON BANK_61476/ABINGTON BANK_61476_20230930.XBRL'\n",
    "schema_file_path = schema_base_path + 'call-report051-2023-09-30-v261.xsd'\n",
    "presentation_file_path = schema_base_path + 'call-report051-2023-09-30-v261-pres.xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from lxml import etree\n",
    "import pandas as pd\n",
    "\n",
    "# Paths to your XBRL file and schema file\n",
    "#xbrl_file_path = '/Users/kris/data_sources/fdic/BulkReports/ABINGTON BANK_61476/ABINGTON BANK_61476_20230930.XBRL'\n",
    "#schema_file_path = '/Users/kris/Downloads/_ 09302023_Form051/call-report051-2023-09-30-v261.xsd'\n",
    "\n",
    "def parse_xsd(schema_path):\n",
    "    with open(schema_path, 'rb') as schema_file:\n",
    "        schema_root = etree.XML(schema_file.read())\n",
    "    return schema_root\n",
    "\n",
    "def parse_xbrl(xbrl_path):\n",
    "    with open(xbrl_path, 'rb') as xbrl_file:\n",
    "        xbrl_root = etree.XML(xbrl_file.read())\n",
    "    return xbrl_root\n",
    "\n",
    "def extract_data_from_xbrl(xbrl_root, schema_root):\n",
    "    namespaces = {'xbrli': 'http://www.xbrl.org/2003/instance'}\n",
    "    context_elements = xbrl_root.xpath('//xbrli:context', namespaces=namespaces)\n",
    "    \n",
    "    data = []\n",
    "    for context in context_elements:\n",
    "        context_id = context.get('id')\n",
    "        data_elements = xbrl_root.xpath(f'//*[@contextRef=\"{context_id}\"]', namespaces=namespaces)\n",
    "        \n",
    "        for element in data_elements:\n",
    "            tag = etree.QName(element).localname\n",
    "            value = element.text\n",
    "            unit = element.get('unitRef', 'N/A')\n",
    "            data.append((context_id, tag, value, unit))\n",
    "    \n",
    "    return data\n",
    "\n",
    "def present_data(data):\n",
    "    df = pd.DataFrame(data, columns=['Context ID', 'Tag', 'Value', 'Unit'])\n",
    "    print(df)\n",
    "\n",
    "def main(xbrl_file_path, schema_file_path):\n",
    "    schema_root = parse_xsd(schema_file_path)\n",
    "    xbrl_root = parse_xbrl(xbrl_file_path)\n",
    "    data = extract_data_from_xbrl(xbrl_root, schema_root)\n",
    "    present_data(data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(xbrl_file_path, schema_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from lxml import etree\n",
    "import pandas as pd\n",
    "\n",
    "# Paths to your XBRL file and schema file\n",
    "#xbrl_file_path = \"path/to/ABINGTON BANK_61476_20230930.XBRL\"\n",
    "#schema_file_path = \"path/to/_09302023_Form051/call-report051-2023-09-30-v261.xsd\"\n",
    "#presentation_file_path = \"path/to/_09302023_Form051/call-report051-2023-09-30-v261-pres.xml\"\n",
    "\n",
    "def parse_xml(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        root = etree.XML(file.read())\n",
    "    return root\n",
    "\n",
    "def validate_xbrl(xbrl_root, schema_root):\n",
    "    schema = etree.XMLSchema(schema_root)\n",
    "    if not schema.validate(xbrl_root):\n",
    "        raise ValueError(\"XBRL file is not valid.\")\n",
    "    return True\n",
    "\n",
    "def extract_data_from_xbrl(xbrl_root):\n",
    "    namespaces = {'xbrli': 'http://www.xbrl.org/2003/instance'}\n",
    "    context_elements = xbrl_root.xpath('//xbrli:context', namespaces=namespaces)\n",
    "    \n",
    "    data = []\n",
    "    for context in context_elements:\n",
    "        context_id = context.get('id')\n",
    "        data_elements = xbrl_root.xpath(f'//*[@contextRef=\"{context_id}\"]', namespaces=namespaces)\n",
    "        \n",
    "        for element in data_elements:\n",
    "            tag = etree.QName(element).localname\n",
    "            value = element.text\n",
    "            unit = element.get('unitRef', 'N/A')\n",
    "            data.append((context_id, tag, value, unit))\n",
    "    \n",
    "    return data\n",
    "\n",
    "def present_data(data):\n",
    "    df = pd.DataFrame(data, columns=['Context ID', 'Tag', 'Value', 'Unit'])\n",
    "    print(df)\n",
    "\n",
    "def main(xbrl_file_path, schema_file_path, presentation_file_path):\n",
    "    schema_root = parse_xml(schema_file_path)\n",
    "    xbrl_root = parse_xml(xbrl_file_path)\n",
    "    #validate_xbrl(xbrl_root, schema_root)\n",
    "    \n",
    "    # Extract data from XBRL instance\n",
    "    data = extract_data_from_xbrl(xbrl_root)\n",
    "    \n",
    "    # Optionally, parse and use the presentation linkbase\n",
    "    presentation_root = parse_xml(presentation_file_path)\n",
    "    \n",
    "    # Present data\n",
    "    present_data(data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(xbrl_file_path, schema_file_path, presentation_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from lxml import etree\n",
    "import pandas as pd\n",
    "\n",
    "# Paths to your XBRL file and schema file\n",
    "#xbrl_file_path = \"path/to/ABINGTON BANK_61476_20230930.XBRL\"\n",
    "#schema_file_path = \"path/to/_09302023_Form051/call-report051-2023-09-30-v261.xsd\"\n",
    "#presentation_file_path = \"path/to/_09302023_Form051/call-report051-2023-09-30-v261-pres.xml\"\n",
    "\n",
    "def parse_xml(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        root = etree.XML(file.read())\n",
    "    return root\n",
    "\n",
    "def validate_xbrl(xbrl_root, schema_root):\n",
    "    schema = etree.XMLSchema(schema_root)\n",
    "    if not schema.validate(xbrl_root):\n",
    "        raise ValueError(\"XBRL file is not valid.\")\n",
    "    return True\n",
    "\n",
    "def extract_data_from_xbrl(xbrl_root):\n",
    "    namespaces = {'xbrli': 'http://www.xbrl.org/2003/instance'}\n",
    "    context_elements = xbrl_root.xpath('//xbrli:context', namespaces=namespaces)\n",
    "    \n",
    "    data = []\n",
    "    for context in context_elements:\n",
    "        context_id = context.get('id')\n",
    "        data_elements = xbrl_root.xpath(f'//*[@contextRef=\"{context_id}\"]', namespaces=namespaces)\n",
    "        \n",
    "        for element in data_elements:\n",
    "            tag = etree.QName(element).localname\n",
    "            value = element.text\n",
    "            unit = element.get('unitRef', 'N/A')\n",
    "            data.append((context_id, tag, value, unit))\n",
    "    \n",
    "    return data\n",
    "\n",
    "def parse_presentation_linkbase(presentation_root):\n",
    "    link_namespace = {'link': 'http://www.xbrl.org/2003/linkbase'}\n",
    "    arcrole = 'http://www.xbrl.org/2003/arcrole/parent-child'\n",
    "    role_refs = presentation_root.xpath('//link:roleRef', namespaces=link_namespace)\n",
    "    \n",
    "    presentations = {}\n",
    "    for role_ref in role_refs:\n",
    "        role_uri = role_ref.get('roleURI')\n",
    "        locators = presentation_root.xpath(f'//link:loc[@xlink:role=\"{role_uri}\"]', namespaces=link_namespace)\n",
    "        arcs = presentation_root.xpath(f'//link:definitionArc[@xlink:arcrole=\"{arcrole}\"]', namespaces=link_namespace)\n",
    "        \n",
    "        elements = []\n",
    "        for arc in arcs:\n",
    "            from_locator = arc.get('from')\n",
    "            to_locator = arc.get('to')\n",
    "            from_element = next((loc for loc in locators if loc.get('label') == from_locator), None)\n",
    "            to_element = next((loc for loc in locators if loc.get('label') == to_locator), None)\n",
    "            if from_element is not None and to_element is not None:\n",
    "                elements.append((from_element.get('{http://www.w3.org/1999/xlink}href'), to_element.get('{http://www.w3.org/1999/xlink}href')))\n",
    "        presentations[role_uri] = elements\n",
    "    \n",
    "    return presentations\n",
    "\n",
    "def present_data(data, presentations):\n",
    "    df = pd.DataFrame(data, columns=['Context ID', 'Tag', 'Value', 'Unit'])\n",
    "    \n",
    "    presentation_order = []\n",
    "    for role_uri, elements in presentations.items():\n",
    "        for parent, child in elements:\n",
    "            presentation_order.append(child.split('#')[-1])\n",
    "    \n",
    "    df['Tag'] = pd.Categorical(df['Tag'], categories=presentation_order, ordered=True)\n",
    "    df.sort_values('Tag', inplace=True)\n",
    "    \n",
    "    print(df)\n",
    "\n",
    "def main(xbrl_file_path, schema_file_path, presentation_file_path):\n",
    "    schema_root = parse_xml(schema_file_path)\n",
    "    xbrl_root = parse_xml(xbrl_file_path)\n",
    "    #validate_xbrl(xbrl_root, schema_root)\n",
    "    \n",
    "    # Extract data from XBRL instance\n",
    "    data = extract_data_from_xbrl(xbrl_root)\n",
    "    \n",
    "    # Parse presentation linkbase\n",
    "    presentation_root = parse_xml(presentation_file_path)\n",
    "    presentations = parse_presentation_linkbase(presentation_root)\n",
    "    \n",
    "    # Present data\n",
    "    present_data(data, presentations)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(xbrl_file_path, schema_file_path, presentation_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from lxml import etree\n",
    "import pandas as pd\n",
    "\n",
    "# Paths to your XBRL file and schema file\n",
    "#xbrl_file_path = \"path/to/ABINGTON BANK_61476_20230930.XBRL\"\n",
    "#schema_file_path = \"path/to/_09302023_Form051/call-report051-2023-09-30-v261.xsd\"\n",
    "#presentation_file_path = \"path/to/_09302023_Form051/call-report051-2023-09-30-v261-pres.xml\"\n",
    "\n",
    "def parse_xml(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        root = etree.XML(file.read())\n",
    "    return root\n",
    "\n",
    "def validate_xbrl(xbrl_root, schema_root):\n",
    "    schema = etree.XMLSchema(schema_root)\n",
    "    if not schema.validate(xbrl_root):\n",
    "        raise ValueError(\"XBRL file is not valid.\")\n",
    "    return True\n",
    "\n",
    "def extract_data_from_xbrl(xbrl_root):\n",
    "    namespaces = {'xbrli': 'http://www.xbrl.org/2003/instance'}\n",
    "    context_elements = xbrl_root.xpath('//xbrli:context', namespaces=namespaces)\n",
    "    \n",
    "    data = []\n",
    "    for context in context_elements:\n",
    "        context_id = context.get('id')\n",
    "        data_elements = xbrl_root.xpath(f'//*[@contextRef=\"{context_id}\"]', namespaces=namespaces)\n",
    "        \n",
    "        for element in data_elements:\n",
    "            tag = etree.QName(element).localname\n",
    "            value = element.text\n",
    "            unit = element.get('unitRef', 'N/A')\n",
    "            data.append((context_id, tag, value, unit))\n",
    "    \n",
    "    return data\n",
    "\n",
    "def parse_presentation_linkbase(presentation_root):\n",
    "    link_namespace = {'link': 'http://www.xbrl.org/2003/linkbase'}\n",
    "    xlink_namespace = {'xlink': 'http://www.w3.org/1999/xlink'}\n",
    "    \n",
    "    locators = {}\n",
    "    for loc in presentation_root.xpath('//link:loc', namespaces=link_namespace):\n",
    "        label = loc.get('{http://www.w3.org/1999/xlink}label')\n",
    "        href = loc.get('{http://www.w3.org/1999/xlink}href')\n",
    "        locators[label] = href.split('#')[-1]\n",
    "    \n",
    "    presentation_order = []\n",
    "    for arc in presentation_root.xpath('//link:presentationArc', namespaces=link_namespace):\n",
    "        from_label = arc.get('from')\n",
    "        to_label = arc.get('to')\n",
    "        if from_label in locators and to_label in locators:\n",
    "            presentation_order.append(locators[to_label])\n",
    "    \n",
    "    return presentation_order\n",
    "\n",
    "def present_data(data, presentation_order):\n",
    "    df = pd.DataFrame(data, columns=['Context ID', 'Tag', 'Value', 'Unit'])\n",
    "    \n",
    "    # Use presentation order to categorize and sort tags\n",
    "    df['Tag'] = pd.Categorical(df['Tag'], categories=presentation_order, ordered=True)\n",
    "    df.sort_values('Tag', inplace=True)\n",
    "    \n",
    "    print(df)\n",
    "\n",
    "def main(xbrl_file_path, schema_file_path, presentation_file_path):\n",
    "    schema_root = parse_xml(schema_file_path)\n",
    "    xbrl_root = parse_xml(xbrl_file_path)\n",
    "    #validate_xbrl(xbrl_root, schema_root)\n",
    "    \n",
    "    # Extract data from XBRL instance\n",
    "    data = extract_data_from_xbrl(xbrl_root)\n",
    "    \n",
    "    # Parse presentation linkbase\n",
    "    presentation_root = parse_xml(presentation_file_path)\n",
    "    presentation_order = parse_presentation_linkbase(presentation_root)\n",
    "    \n",
    "    # Present data\n",
    "    present_data(data, presentation_order)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(xbrl_file_path, schema_file_path, presentation_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from lxml import etree\n",
    "import pandas as pd\n",
    "\n",
    "# Paths to your XBRL file and schema file\n",
    "#xbrl_file_path = \"/path/to/ABINGTON BANK_61476_20230930.XBRL\"\n",
    "#schema_file_path = \"/path/to/_09302023_Form051/call-report051-2023-09-30-v261.xsd\"\n",
    "#presentation_file_path = \"/mnt/data/call-report051-2023-09-30-v261-pres.xml\"\n",
    "\n",
    "def parse_xml(file_path):\n",
    "    print(f\"Parsing XML file: {file_path}\")\n",
    "    with open(file_path, 'rb') as file:\n",
    "        root = etree.XML(file.read())\n",
    "    return root\n",
    "\n",
    "def validate_xbrl(xbrl_root, schema_root):\n",
    "    print(\"Validating XBRL file\")\n",
    "    schema = etree.XMLSchema(schema_root)\n",
    "    if not schema.validate(xbrl_root):\n",
    "        raise ValueError(\"XBRL file is not valid.\")\n",
    "    return True\n",
    "\n",
    "def extract_data_from_xbrl(xbrl_root):\n",
    "    print(\"Extracting data from XBRL\")\n",
    "    namespaces = {'xbrli': 'http://www.xbrl.org/2003/instance'}\n",
    "    context_elements = xbrl_root.xpath('//xbrli:context', namespaces=namespaces)\n",
    "    \n",
    "    data = []\n",
    "    for context in context_elements:\n",
    "        context_id = context.get('id')\n",
    "        data_elements = xbrl_root.xpath(f'//*[@contextRef=\"{context_id}\"]', namespaces=namespaces)\n",
    "        \n",
    "        for element in data_elements:\n",
    "            tag = etree.QName(element).localname\n",
    "            value = element.text\n",
    "            unit = element.get('unitRef', 'N/A')\n",
    "            data.append((context_id, tag, value, unit))\n",
    "    \n",
    "    return data\n",
    "\n",
    "def parse_presentation_linkbase(presentation_root):\n",
    "    print(\"Parsing presentation linkbase\")\n",
    "    link_namespace = {'link': 'http://www.xbrl.org/2003/linkbase'}\n",
    "    xlink_namespace = {'xlink': 'http://www.w3.org/1999/xlink'}\n",
    "    \n",
    "    locators = {}\n",
    "    for loc in presentation_root.xpath('//link:loc', namespaces=link_namespace):\n",
    "        label = loc.get('{http://www.w3.org/1999/xlink}label')\n",
    "        href = loc.get('{http://www.w3.org/1999/xlink}href')\n",
    "        locators[label] = href.split('#')[-1]\n",
    "    \n",
    "    presentation_order = []\n",
    "    hierarchy = {}\n",
    "    for arc in presentation_root.xpath('//link:presentationArc', namespaces=link_namespace):\n",
    "        from_label = arc.get('from')\n",
    "        to_label = arc.get('to')\n",
    "        if from_label in locators and to_label in locators:\n",
    "            presentation_order.append(locators[to_label])\n",
    "            if locators[from_label] not in hierarchy:\n",
    "                hierarchy[locators[from_label]] = []\n",
    "            hierarchy[locators[from_label]].append(locators[to_label])\n",
    "    \n",
    "    return presentation_order, hierarchy\n",
    "\n",
    "def present_data(data, presentation_order, hierarchy):\n",
    "    print(\"Presenting data\")\n",
    "    df = pd.DataFrame(data, columns=['Context ID', 'Tag', 'Value', 'Unit'])\n",
    "    \n",
    "    # Use presentation order to categorize and sort tags\n",
    "    df['Tag'] = pd.Categorical(df['Tag'], categories=presentation_order, ordered=True)\n",
    "    df.sort_values('Tag', inplace=True)\n",
    "    \n",
    "    # Display the hierarchical structure\n",
    "    for parent, children in hierarchy.items():\n",
    "        print(f\"{parent}:\")\n",
    "        for child in children:\n",
    "            print(f\"  - {child}\")\n",
    "    \n",
    "    print(df)\n",
    "\n",
    "def main(xbrl_file_path, schema_file_path, presentation_file_path):\n",
    "    print(\"Starting main process\")\n",
    "    schema_root = parse_xml(schema_file_path)\n",
    "    xbrl_root = parse_xml(xbrl_file_path)\n",
    "    #validate_xbrl(xbrl_root, schema_root)\n",
    "    \n",
    "    # Extract data from XBRL instance\n",
    "    data = extract_data_from_xbrl(xbrl_root)\n",
    "    \n",
    "    # Parse presentation linkbase\n",
    "    presentation_root = parse_xml(presentation_file_path)\n",
    "    presentation_order, hierarchy = parse_presentation_linkbase(presentation_root)\n",
    "    \n",
    "    # Present data\n",
    "    present_data(data, presentation_order, hierarchy)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(xbrl_file_path, schema_file_path, presentation_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from lxml import etree\n",
    "import pandas as pd\n",
    "\n",
    "# Paths to your XBRL file, schema file, and taxonomy files\n",
    "bulkReport_path = '/Users/kris/data_sources/fdic/BulkReports/'\n",
    "schema_path = '/Users/kris/Desktop/BankRegScraper/taxonomy/Call_ 12312023_Form051/'\n",
    "\n",
    "xbrl_file_path = os.path.join(bulkReport_path, \"ABINGTON BANK_61476/ABINGTON BANK_61476_20231231.XBRL\")\n",
    "schema_files = [\n",
    "    os.path.join(schema_path, \"call-report051-2023-12-31-v265.xsd\"),\n",
    "    os.path.join(schema_path, \"concepts.xsd\"),\n",
    "    os.path.join(schema_path, \"ffiec-instance-2004-06-10.xsd\"),\n",
    "    os.path.join(schema_path, \"ffiec-linkbase-2004-05-14.xsd\"),\n",
    "    os.path.join(schema_path, \"mdr_schema.xsd\")\n",
    "]\n",
    "linkbase_files = {\n",
    "    \"presentation\": os.path.join(schema_path, \"call-report051-2023-12-31-v265-pres.xml\"),\n",
    "    \"calculation\": os.path.join(schema_path, \"call-report051-2023-12-31-v265-calc.xml\"),\n",
    "    \"definition\": os.path.join(schema_path, \"call-report051-2023-12-31-v265-def.xml\"),\n",
    "    \"reference\": os.path.join(schema_path, \"call-report051-2023-12-31-v265-ref.xml\"),\n",
    "    \"label\": os.path.join(schema_path, \"call-report051-2023-12-31-v265-instr.xml\"),\n",
    "    \"ec-mess\": os.path.join(schema_path, \"call-report051-2023-12-31-v265-ec-mess.xml\"),\n",
    "    \"ec\": os.path.join(schema_path, \"call-report051-2023-12-31-v265-ec.xml\")\n",
    "}\n",
    "\n",
    "def parse_xml(file_path):\n",
    "    print(f\"Parsing XML file: {file_path}\")\n",
    "    try:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            root = etree.XML(file.read())\n",
    "        return root\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to parse XML file: {file_path}. Error: {e}\")\n",
    "        raise\n",
    "\n",
    "def validate_xbrl(xbrl_root, schema_roots):\n",
    "    print(\"Validating XBRL file\")\n",
    "    for schema_root in schema_roots:\n",
    "        print(f\"Validating against schema: {schema_root}\")\n",
    "        schema = etree.XMLSchema(schema_root)\n",
    "        if not schema.validate(xbrl_root):\n",
    "            print(f\"Validation failed against schema: {schema_root}\")\n",
    "            raise ValueError(\"XBRL file is not valid against the schema.\")\n",
    "        else:\n",
    "            print(f\"Validation succeeded against schema: {schema_root}\")\n",
    "    print(\"Validation completed\")\n",
    "    return True\n",
    "\n",
    "def extract_data_from_xbrl(xbrl_root):\n",
    "    print(\"Extracting data from XBRL\")\n",
    "    namespaces = {'xbrli': 'http://www.xbrl.org/2003/instance'}\n",
    "    context_elements = xbrl_root.xpath('//xbrli:context', namespaces=namespaces)\n",
    "    \n",
    "    data = []\n",
    "    for context in context_elements:\n",
    "        context_id = context.get('id')\n",
    "        data_elements = xbrl_root.xpath(f'//*[@contextRef=\"{context_id}\"]', namespaces=namespaces)\n",
    "        \n",
    "        for element in data_elements:\n",
    "            tag = etree.QName(element).localname\n",
    "            value = element.text\n",
    "            unit = element.get('unitRef', 'N/A')\n",
    "            data.append((context_id, tag, value, unit))\n",
    "    \n",
    "    return data\n",
    "\n",
    "def parse_linkbase(linkbase_file, arc_tag):\n",
    "    print(f\"Parsing {arc_tag} linkbase\")\n",
    "    link_namespace = {'link': 'http://www.xbrl.org/2003/linkbase'}\n",
    "    \n",
    "    root = parse_xml(linkbase_file)\n",
    "    locators = {}\n",
    "    for loc in root.xpath('//link:loc', namespaces=link_namespace):\n",
    "        label = loc.get('{http://www.w3.org/1999/xlink}label')\n",
    "        href = loc.get('{http://www.w3.org/1999/xlink}href')\n",
    "        locators[label] = href.split('#')[-1]\n",
    "    \n",
    "    arcs = []\n",
    "    for arc in root.xpath(f'//link:{arc_tag}Arc', namespaces=link_namespace):\n",
    "        from_label = arc.get('from')\n",
    "        to_label = arc.get('to')\n",
    "        if from_label in locators and to_label in locators:\n",
    "            arcs.append((locators[from_label], locators[to_label]))\n",
    "    \n",
    "    return locators, arcs\n",
    "\n",
    "def parse_presentation_linkbase(presentation_file):\n",
    "    locators, presentation_arcs = parse_linkbase(presentation_file, 'presentation')\n",
    "    presentation_order = []\n",
    "    hierarchy = {}\n",
    "    for parent, child in presentation_arcs:\n",
    "        presentation_order.append(child)\n",
    "        if parent not in hierarchy:\n",
    "            hierarchy[parent] = []\n",
    "        hierarchy[parent].append(child)\n",
    "    \n",
    "    return presentation_order, hierarchy\n",
    "\n",
    "def parse_calculation_linkbase(calculation_file):\n",
    "    locators, calculation_arcs = parse_linkbase(calculation_file, 'calculation')\n",
    "    calculations = {}\n",
    "    for parent, child in calculation_arcs:\n",
    "        if parent not in calculations:\n",
    "            calculations[parent] = []\n",
    "        calculations[parent].append(child)\n",
    "    \n",
    "    return calculations\n",
    "\n",
    "# Function to parse the definition linkbase with additional debug statements\n",
    "def parse_definition_linkbase(definition_file):\n",
    "    print(f\"Parsing definition linkbase: {definition_file}\")\n",
    "    link_namespace = {'link': 'http://www.xbrl.org/2003/linkbase'}\n",
    "    \n",
    "    try:\n",
    "        root = etree.parse(definition_file)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to parse definition linkbase file: {definition_file}. Error: {e}\")\n",
    "        return {}\n",
    "    \n",
    "    locators = {}\n",
    "    for loc in root.xpath('//link:loc', namespaces=link_namespace):\n",
    "        label = loc.get('{http://www.w3.org/1999/xlink}label')\n",
    "        href = loc.get('{http://www.w3.org/1999/xlink}href')\n",
    "        locators[label] = href.split('#')[-1]\n",
    "    \n",
    "    print(f\"Locators found: {locators}\")\n",
    "    \n",
    "    definition_arcs = []\n",
    "    for arc in root.xpath('//link:defArc', namespaces=link_namespace):\n",
    "        from_label = arc.get('from')\n",
    "        to_label = arc.get('to')\n",
    "        if from_label in locators and to_label in locators:\n",
    "            definition_arcs.append((locators[from_label], locators[to_label]))\n",
    "    \n",
    "    print(f\"Definition arcs found: {definition_arcs}\")\n",
    "    \n",
    "    definitions = {}\n",
    "    for parent, child in definition_arcs:\n",
    "        if parent not in definitions:\n",
    "            definitions[parent] = []\n",
    "        definitions[parent].append(child)\n",
    "    \n",
    "    print(f\"Definitions: {definitions}\")\n",
    "    return definitions\n",
    "    \n",
    "def parse_additional_linkbases(additional_file):\n",
    "    # Add logic to parse additional linkbases if needed\n",
    "    pass\n",
    "\n",
    "def present_data(data, presentation_order, hierarchy, calculations, definitions):\n",
    "    print(\"Presenting data\")\n",
    "    df = pd.DataFrame(data, columns=['Context ID', 'Tag', 'Value', 'Unit'])\n",
    "    \n",
    "    # Use presentation order to categorize and sort tags\n",
    "    df['Tag'] = pd.Categorical(df['Tag'], categories=presentation_order, ordered=True)\n",
    "    df.sort_values('Tag', inplace=True)\n",
    "    \n",
    "    # Display the hierarchical structure\n",
    "    print(\"Hierarchy:\")\n",
    "    for parent, children in hierarchy.items():\n",
    "        print(f\"{parent}:\")\n",
    "        for child in children:\n",
    "            print(f\"  - {child}\")\n",
    "    \n",
    "    # Display calculation relationships\n",
    "    print(\"\\nCalculations:\")\n",
    "    for parent, children in calculations.items():\n",
    "        print(f\"{parent}:\")\n",
    "        for child in children:\n",
    "            print(f\"  - {child}\")\n",
    "    \n",
    "    # Display definition relationships\n",
    "    print(\"\\nDefinitions:\")\n",
    "    for parent, children in definitions.items():\n",
    "        print(f\"{parent}:\")\n",
    "        for child in children:\n",
    "            print(f\"  - {child}\")\n",
    "    \n",
    "    print(\"\\nData:\")\n",
    "    print(df)\n",
    "\n",
    "def main(xbrl_file_path, schema_files, linkbase_files):\n",
    "    print(\"Starting main process\")\n",
    "    schema_roots = [parse_xml(file_path) for file_path in schema_files]\n",
    "    xbrl_root = parse_xml(xbrl_file_path)\n",
    "    \n",
    "    #validate_xbrl(xbrl_root, schema_roots)\n",
    "    \n",
    "    # Extract data from XBRL instance\n",
    "    data = extract_data_from_xbrl(xbrl_root)\n",
    "    \n",
    "    # Parse linkbases\n",
    "    presentation_order, hierarchy = parse_presentation_linkbase(linkbase_files['presentation'])\n",
    "    calculations = parse_calculation_linkbase(linkbase_files['calculation'])\n",
    "    definitions = parse_definition_linkbase(linkbase_files['definition'])\n",
    "    \n",
    "    # Parse additional linkbases if needed\n",
    "    parse_additional_linkbases(linkbase_files['ec-mess'])\n",
    "    parse_additional_linkbases(linkbase_files['ec'])\n",
    "    \n",
    "    # Present data\n",
    "    present_data(data, presentation_order, hierarchy, calculations, definitions)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(xbrl_file_path, schema_files, linkbase_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from lxml import etree\n",
    "import pandas as pd\n",
    "\n",
    "# Paths to your XBRL file, schema file, and taxonomy files\n",
    "bulkReport_path = '/Users/kris/data_sources/fdic/BulkReports/'\n",
    "schema_path = '/Users/kris/Desktop/BankRegScraper/taxonomy/Call_ 12312023_Form051/'\n",
    "\n",
    "xbrl_file_path = os.path.join(bulkReport_path, \"ABINGTON BANK_61476/ABINGTON BANK_61476_20230930.XBRL\")\n",
    "schema_files = [\n",
    "    os.path.join(schema_path, \"call-report051-2023-12-31-v265.xsd\"),\n",
    "    os.path.join(schema_path, \"concepts.xsd\"),\n",
    "    os.path.join(schema_path, \"ffiec-instance-2004-06-10.xsd\"),\n",
    "    os.path.join(schema_path, \"ffiec-linkbase-2004-05-14.xsd\"),\n",
    "    os.path.join(schema_path, \"mdr_schema.xsd\")\n",
    "]\n",
    "linkbase_files = {\n",
    "    \"presentation\": os.path.join(schema_path, \"call-report051-2023-12-31-v265-pres.xml\"),\n",
    "    \"calculation\": os.path.join(schema_path, \"call-report051-2023-12-31-v265-calc.xml\"),\n",
    "    \"definition\": os.path.join(schema_path, \"call-report051-2023-12-31-v265-def.xml\"),\n",
    "    \"reference\": os.path.join(schema_path, \"call-report051-2023-12-31-v265-ref.xml\"),\n",
    "    \"label\": os.path.join(schema_path, \"call-report051-2023-12-31-v265-instr.xml\"),\n",
    "    \"ec-mess\": os.path.join(schema_path, \"call-report051-2023-12-31-v265-ec-mess.xml\"),\n",
    "    \"ec\": os.path.join(schema_path, \"call-report051-2023-12-31-v265-ec.xml\")\n",
    "}\n",
    "\n",
    "def parse_xml(file_path):\n",
    "    print(f\"Parsing XML file: {file_path}\")\n",
    "    try:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            root = etree.XML(file.read())\n",
    "        return root\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to parse XML file: {file_path}. Error: {e}\")\n",
    "        raise\n",
    "\n",
    "def validate_xbrl(xbrl_root, schema_roots):\n",
    "    print(\"Validating XBRL file\")\n",
    "    for schema_root in schema_roots:\n",
    "        print(f\"Validating against schema: {schema_root}\")\n",
    "        schema = etree.XMLSchema(schema_root)\n",
    "        if not schema.validate(xbrl_root):\n",
    "            print(f\"Validation failed against schema: {schema_root}\")\n",
    "            raise ValueError(\"XBRL file is not valid against the schema.\")\n",
    "        else:\n",
    "            print(f\"Validation succeeded against schema: {schema_root}\")\n",
    "    print(\"Validation completed\")\n",
    "    return True\n",
    "\n",
    "def extract_data_from_xbrl(xbrl_root):\n",
    "    print(\"Extracting data from XBRL\")\n",
    "    namespaces = {'xbrli': 'http://www.xbrl.org/2003/instance'}\n",
    "    context_elements = xbrl_root.xpath('//xbrli:context', namespaces=namespaces)\n",
    "    \n",
    "    data = []\n",
    "    for context in context_elements:\n",
    "        context_id = context.get('id')\n",
    "        data_elements = xbrl_root.xpath(f'//*[@contextRef=\"{context_id}\"]', namespaces=namespaces)\n",
    "        \n",
    "        for element in data_elements:\n",
    "            tag = etree.QName(element).localname\n",
    "            value = element.text\n",
    "            unit = element.get('unitRef', 'N/A')\n",
    "            data.append((context_id, tag, value, unit))\n",
    "    \n",
    "    return data\n",
    "\n",
    "def parse_linkbase(linkbase_file, arc_tag):\n",
    "    print(f\"Parsing {arc_tag} linkbase: {linkbase_file}\")\n",
    "    link_namespace = {'link': 'http://www.xbrl.org/2003/linkbase'}\n",
    "    \n",
    "    root = parse_xml(linkbase_file)\n",
    "    locators = {}\n",
    "    for loc in root.xpath('//link:loc', namespaces=link_namespace):\n",
    "        label = loc.get('{http://www.w3.org/1999/xlink}label')\n",
    "        href = loc.get('{http://www.w3.org/1999/xlink}href')\n",
    "        locators[label] = href.split('#')[-1]\n",
    "    \n",
    "    print(f\"Locators found: {locators}\")\n",
    "    \n",
    "    arcs = []\n",
    "    for arc in root.xpath(f'//link:{arc_tag}Arc', namespaces=link_namespace):\n",
    "        from_label = arc.get('from')\n",
    "        to_label = arc.get('to')\n",
    "        #print(f\"Processing {arc_tag}Arc from {from_label} to {to_label}\")\n",
    "        if from_label in locators and to_label in locators:\n",
    "            arcs.append((locators[from_label], locators[to_label]))\n",
    "    \n",
    "    print(f\"Arcs found in {arc_tag} linkbase: {arcs}\")\n",
    "    return locators, arcs\n",
    "\n",
    "def parse_presentation_linkbase(presentation_file):\n",
    "    locators, presentation_arcs = parse_linkbase(presentation_file, 'presentation')\n",
    "    presentation_order = []\n",
    "    hierarchy = {}\n",
    "    for parent, child in presentation_arcs:\n",
    "        presentation_order.append(child)\n",
    "        if parent not in hierarchy:\n",
    "            hierarchy[parent] = []\n",
    "        hierarchy[parent].append(child)\n",
    "    \n",
    "    return presentation_order, hierarchy\n",
    "\n",
    "def parse_calculation_linkbase(calculation_file):\n",
    "    print(f\"Parsing calculation linkbase: {calculation_file}\")\n",
    "    locators, calculation_arcs = parse_linkbase(calculation_file, 'calculation')\n",
    "    calculations = {}\n",
    "    for parent, child in calculation_arcs:\n",
    "        if parent not in calculations:\n",
    "            calculations[parent] = []\n",
    "        calculations[parent].append(child)\n",
    "    \n",
    "    print(f\"Calculations: {calculations}\")\n",
    "    return calculations\n",
    "\n",
    "def parse_definition_linkbase(definition_file):\n",
    "    print(f\"Parsing definition linkbase: {definition_file}\")\n",
    "    link_namespace = {'link': 'http://www.xbrl.org/2003/linkbase'}\n",
    "    \n",
    "    root = parse_xml(definition_file)\n",
    "    locators = {}\n",
    "    for loc in root.xpath('//link:loc', namespaces=link_namespace):\n",
    "        label = loc.get('{http://www.w3.org/1999/xlink}label')\n",
    "        href = loc.get('{http://www.w3.org/1999/xlink}href')\n",
    "        locators[label] = href.split('#')[-1]\n",
    "    \n",
    "    print(f\"Locators found: {locators}\")\n",
    "    \n",
    "    definition_arcs = []\n",
    "    for arc in root.xpath('//link:defArc', namespaces=link_namespace):\n",
    "        from_label = arc.get('from')\n",
    "        to_label = arc.get('to')\n",
    "        print(f\"Processing defArc from {from_label} to {to_label}\")\n",
    "        if from_label in locators and to_label in locators:\n",
    "            definition_arcs.append((locators[from_label], locators[to_label]))\n",
    "    \n",
    "    print(f\"Definition arcs found: {definition_arcs}\")\n",
    "    \n",
    "    definitions = {}\n",
    "    for parent, child in definition_arcs:\n",
    "        if parent not in definitions:\n",
    "            definitions[parent] = []\n",
    "        definitions[parent].append(child)\n",
    "    \n",
    "    print(f\"Definitions: {definitions}\")\n",
    "    return definitions\n",
    "\n",
    "def parse_additional_linkbases(additional_file):\n",
    "    # Add logic to parse additional linkbases if needed\n",
    "    pass\n",
    "\n",
    "def present_data(data, presentation_order, hierarchy, calculations=None, definitions=None):\n",
    "    print(\"Presenting data\")\n",
    "    df = pd.DataFrame(data, columns=['Context ID', 'Tag', 'Value', 'Unit'])\n",
    "    \n",
    "    # Use presentation order to categorize and sort tags\n",
    "    df['Tag'] = pd.Categorical(df['Tag'], categories=presentation_order, ordered=True)\n",
    "    df.sort_values('Tag', inplace=True)\n",
    "    \n",
    "    # Display the hierarchical structure\n",
    "    print(\"Hierarchy:\")\n",
    "    for parent, children in hierarchy.items():\n",
    "        print(f\"{parent}: {children}\")\n",
    "    \n",
    "    # Display calculations\n",
    "    if calculations:\n",
    "        print(\"\\nCalculations:\")\n",
    "        for parent, children in calculations.items():\n",
    "            print(f\"{parent}: {children}\")\n",
    "    \n",
    "    # Display definitions\n",
    "    if definitions:\n",
    "        print(\"\\nDefinitions:\")\n",
    "        for parent, children in definitions.items():\n",
    "            print(f\"{parent}: {children}\")\n",
    "\n",
    "def main():\n",
    "    # Parsing the provided definition linkbase\n",
    "    definitions = parse_definition_linkbase(linkbase_files['definition'])\n",
    "\n",
    "    # Parsing other necessary linkbases\n",
    "    calculations = parse_calculation_linkbase(linkbase_files['calculation'])\n",
    "    presentation_order, hierarchy = parse_presentation_linkbase(linkbase_files['presentation'])\n",
    "\n",
    "    # Parsing and validating XBRL data\n",
    "    xbrl_root = parse_xml(xbrl_file_path)\n",
    "    schema_roots = [parse_xml(schema_file) for schema_file in schema_files]\n",
    "    #validate_xbrl(xbrl_root, schema_roots)\n",
    "\n",
    "    # Extracting data from XBRL\n",
    "    data = extract_data_from_xbrl(xbrl_root)\n",
    "\n",
    "    # Presenting data\n",
    "    present_data(data, presentation_order, hierarchy, calculations=calculations, definitions=definitions)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import FDIC.wsio as wsio\n",
    "\n",
    "\n",
    "filepath = '/Users/kris/data_sources/fdic/Src/Taxonomies/Call_ 12312023_Form051/call-report051-2023-12-31-v265-cap.xml'\n",
    "\n",
    "def ParseXBRL(self, filepath):\n",
    "    \"\"\"\n",
    "    Parses Call Reports to extract all MDRM item and value from XBRL file\n",
    "    \"\"\"\n",
    "    # Parse XML\n",
    "    tree  = ET.parse(filepath)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    report_values = []\n",
    "    for child in root:\n",
    "        #print(child.tag, child.attrib)\n",
    "        code = child.tag[child.tag.find('}')+1:]\n",
    "        if len(code) == 8:\n",
    "            report_values.append([code, child.text])\n",
    "\n",
    "    parsed_df = pd.DataFrame(report_values, columns = ['MDRM_Item','Value'])\n",
    "    MDRM_df = wsio.ReadCSV(paths.folder_Orig + paths.filename_MDRM)\n",
    "    full_df=parsed_df.set_index('MDRM_Item').join(MDRM_df.set_index('MDRM_Item'))\n",
    "    #print(len(tot[pd.isnull(tot.Item_Name)]))\n",
    "    full_df.reset_index(level=0, inplace=True)\n",
    "    return full_df\n",
    "\n",
    "\n",
    "tree  = ET.parse(filepath)\n",
    "root = tree.getroot()\n",
    "\n",
    "report_values = []\n",
    "for child in root:\n",
    "    #print(child.tag, child.attrib)\n",
    "    code = child.tag[child.tag.find('}')+1:]\n",
    "    if len(code) == 8:\n",
    "        report_values.append([code, child.text])\n",
    "\n",
    "parsed_df = pd.DataFrame(report_values, columns = ['MDRM_Item','Value'])\n",
    "MDRM_df = wsio.ReadCSV(paths.folder_Orig + paths.filename_MDRM)\n",
    "full_df=parsed_df.set_index('MDRM_Item').join(MDRM_df.set_index('MDRM_Item'))\n",
    "#print(len(tot[pd.isnull(tot.Item_Name)]))\n",
    "full_df.reset_index(level=0, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pres_file_path = '/Users/kris/data_sources/fdic/Src/Taxonomies/Call_ 12312023_Form051/call-report051-2023-12-31-v265-pres.xml'\n",
    "cap_file_path = '/Users/kris/data_sources/fdic/Src/Taxonomies/Call_ 12312023_Form051/call-report051-2023-12-31-v265-cap.xml'\n",
    "\n",
    "path = '/Users/kris/data_sources/fdic/Src/Taxonomies/Call_ 12312023_Form051/call-report051-2023-12-31-v265-pres.xml'\n",
    "pres_search_string = '/*pres.xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_text(input_data):\n",
    "    texts_to_remove = [\n",
    "        '{http://www.w3.org/1999/xlink}',\n",
    "        '{http://www.xbrl.org/2003/linkbase}'\n",
    "        ]\n",
    "    \n",
    "    def removeText(text, texts_to_remove):\n",
    "        for string in texts_to_remove:\n",
    "            text = text.replace(string, \"\")\n",
    "        return text\n",
    "\n",
    "    if isinstance(input_data, str):\n",
    "        return removeText(input_data, texts_to_remove)\n",
    "    elif isinstance(input_data, dict):\n",
    "        return {removeText(key, texts_to_remove) if isinstance(key, str) else key: removeText(value, texts_to_remove) if isinstance(value, str) else value\n",
    "                for key, value in input_data.items()}\n",
    "    else:\n",
    "        raise TypeError(\"Input data must be either a str or a dict\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import FDIC.constants as paths\n",
    "import pandas as pd\n",
    "import glob\n",
    "import regex as re\n",
    "\n",
    "\n",
    "def safe_get_prefix(attribute, key, prefix_length=3):\n",
    "    value = attribute.get(key)\n",
    "    return value[:prefix_length] if value else None\n",
    "\n",
    "\n",
    "def get_ReportAndDate(filename) -> tuple[str, str]:\n",
    "\n",
    "    #File is formatted as follows:  \"call-report051-2023-12-31-v265-pres.xml\"\n",
    "    #search_pattern = '([^/]+$)'\n",
    "    search_pattern = 'Form[0-9]{3}'\n",
    "    fn_report = re.search(search_pattern,filename).group()\n",
    "    #fn_date_split = filename.replace('call-report','').split('-')\n",
    "    fn_date_split = filename.split('-')\n",
    "    fn_date = f'{fn_date_split[2]}-{fn_date_split[3]}-{fn_date_split[4]}' if (fn_date_split[2] and fn_date_split[3] and fn_date_split[4]) else None\n",
    "    return (fn_report, fn_date)\n",
    "\n",
    "\n",
    "# Print the structure of the presentation file\n",
    "# These taxonomies have a very particular and nuanced structure\n",
    "def parse_pres(input_path= paths.localPath + paths.folder_Taxonomies, input_search_pattern= '*/*-pres.xml', input_df= None, output_filepath= paths.folder_Orig + paths.filename_MDRM):\n",
    "\n",
    "    pres_dtype = {'from':str, 'to':str, 'date':str, 'report':str}\n",
    "    pres_df = pd.DataFrame(columns=['from', 'to', 'date', 'report']) #, dtype=pres_dtype)\n",
    "\n",
    "    for file_path in glob.glob(input_path + input_search_pattern):\n",
    "\n",
    "        file_df = pd.DataFrame(columns=['from', 'to', 'date', 'report']) #, dtype=pres_dtype)\n",
    "\n",
    "        # Parse the presentation linkbase file\n",
    "        xml_tree = ET.parse(file_path)\n",
    "        xml_root = xml_tree.getroot()\n",
    "\n",
    "        for child in xml_root:\n",
    "            #print(child.tag, child.attrib)\n",
    "\n",
    "            if child.tag[child.tag.find('}')+1:] == 'presentationLink':\n",
    "                for i,ch in enumerate(child):\n",
    "\n",
    "                    tag = remove_text(ch.tag)\n",
    "                    attrib = remove_text(ch.attrib)\n",
    "                    label_prefix = safe_get_prefix(attrib, 'label')\n",
    "                    from_prefix = safe_get_prefix(attrib, 'from')\n",
    "                    to_prefix = safe_get_prefix(attrib, 'to')\n",
    "\n",
    "                    if tag == 'presentationArc' and (label_prefix == 'cc_' or to_prefix == 'cc_'):\n",
    "                        #att_label = attrib.get(\"label\", \"N/A\")\n",
    "                        att_from = attrib.get('from', 'N/A')\n",
    "                        att_to = attrib.get(\"to\", \"N/A\")\n",
    "                        if att_from:\n",
    "                            att_report, att_date = get_ReportAndDate(file_path) \n",
    "                            file_data = pd.DataFrame({'from': att_from, 'to': att_to, 'date':att_date, 'report':att_report}, index=[0]) #, dtype=pres_dtype)\n",
    "                            file_df = pd.concat([file_df, file_data])\n",
    "        \n",
    "        pres_df = pd.concat([pres_df, file_df], ignore_index=True)\n",
    "    pres_df.to_csv(output_filepath, sep=',', quotechar='\"', index= False)  \n",
    "\n",
    "\n",
    "\n",
    "def parse_cap(input_path= paths.localPath+paths.folder_Taxonomies, input_search_pattern = '*/*-cap.xml', input_df= None, output_filepath= paths.folder_Orig+paths.filename_MDRM):\n",
    "    cap_dtype = {'from':str, 'to':str, 'label':str, 'text':str}\n",
    "    cap_df = pd.DataFrame(columns=['from', 'to', 'label', 'text']) #, dtype=cap_dtype)\n",
    "\n",
    "    for file_path in glob.glob(input_path + input_search_pattern):\n",
    "\n",
    "        file_df = pd.DataFrame(columns=['from', 'to', 'label', 'text']) #, dtype=cap_dtype)\n",
    "\n",
    "        # Parse the presentation linkbase file\n",
    "        xml_tree = ET.parse(file_path)\n",
    "        xml_root = xml_tree.getroot()\n",
    "\n",
    "        for child in xml_root:\n",
    "            #print(child.tag, child.attrib)\n",
    "\n",
    "            if child.tag[child.tag.find('}')+1:] == 'labelLink':\n",
    "                for i,ch in enumerate(child):\n",
    "\n",
    "                    tag = remove_text(ch.tag)\n",
    "                    attrib = remove_text(ch.attrib)\n",
    "\n",
    "                    if tag == 'labelArc':\n",
    "                        att_from = attrib.get('from', 'N/A')\n",
    "                        att_to = attrib.get('to', 'N/A')\n",
    "                        \n",
    "                    if tag == 'label':\n",
    "                        att_label = attrib.get('label', 'N/A')\n",
    "                        att_text = ch.text\n",
    "                        \n",
    "                        if att_label:\n",
    "                            file_data = pd.DataFrame({'from': att_from, 'to': att_to, 'label':att_label, 'text':att_text}, index=[0]) #, dtype=pres_dtype)\n",
    "                            file_df = pd.concat([file_df, file_data])\n",
    "                            #Reset values of all attribute variables\n",
    "                            att_from, att_to, att_label, att_text = None, None, None, None\n",
    "\n",
    "\n",
    "                    #[2 iterations to fill each of these]\n",
    "                    # check row label = labelArc\n",
    "                        #set values\n",
    "                    # check row label = label\n",
    "                        #set values\n",
    "\n",
    "                    #[Run every 2 iterations or when all vals are full??]\n",
    "                    # if labelArc or label\n",
    "                        #set df values; append to df\n",
    "\n",
    "        cap_df = pd.concat([cap_df, file_df], ignore_index=True)\n",
    "    cap_df.to_csv(output_filepath, sep=',', quotechar='\"', index= False)  \n",
    "\n",
    "\n",
    "\n",
    "def merge_dataframes(dataframes_and_mergeColumns: list[tuple[pd.DataFrame, str]], is_filepath=False, how='inner'):\n",
    "    \"\"\"\n",
    "    Merges multiple dataframes into a single dataframe based on specified columns.\n",
    "\n",
    "    :param dataframes_and_mergeColumns: List of tuples containing either DataFrames or file paths and their corresponding\n",
    "                                merge column names. \n",
    "                                Format: [(df_or_path, merge_column), (df_or_path, merge_column), ...]\n",
    "    :param filepaths: Boolean indicating if the input is a list of file paths. Default is False.\n",
    "    :param how: Type of merge to be performed. Default is 'inner'. Other options: 'outer', 'left', 'right'.\n",
    "    :return: Merged pandas DataFrame.\n",
    "    \"\"\"\n",
    "    if is_filepath:\n",
    "        # Load DataFrames from CSV files\n",
    "        dataframes = [(pd.read_csv(df_or_path), col) for df_or_path, col in dataframes_and_mergeColumns]\n",
    "    else:\n",
    "        dataframes = dataframes_and_mergeColumns\n",
    "\n",
    "    # Initialize merged_df with the first DataFrame and its corresponding merge column\n",
    "    merged_df, left_on = dataframes[0]\n",
    "\n",
    "    for df, right_on in dataframes[1:]:\n",
    "        # Merge on specified columns from each DataFrame\n",
    "        merged_df = pd.merge(merged_df, df, left_on=left_on, right_on=right_on, how=how)\n",
    "        left_on = right_on  # Update left_on for the next merge\n",
    "\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "# Example usage:\n",
    "# If you have a list of file paths and their corresponding columns:\n",
    "# merged_df = merge_dataframes([(\"file1.csv\", \"col1\"), (\"file2.csv\", \"col2\"), (\"file3.csv\", \"col3\")],\n",
    "#                              filepaths=True)\n",
    "\n",
    "# If you have a list of DataFrames and their corresponding columns:\n",
    "# merged_df = merge_dataframes([(df1, \"col1\"), (df2, \"col2\"), (df3, \"col3\")],\n",
    "#                              filepaths=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cap TEST\n",
    "#path = paths.localPath + paths.folder_Taxonomies\n",
    "#file_search_pattern = '*/*-cap.xml'\n",
    "file_search_pattern = ''\n",
    "path = '/Users/kris/data_sources/fdic/Src/Taxonomies/Call_ 12312023_Form051/call-report051-2023-12-31-v265-cap.xml'\n",
    "output = 'FDIC/MDRM_cap_test.csv'\n",
    "\n",
    "parse_cap(input_path=path, input_search_pattern=file_search_pattern, output_filepath=output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pres_file = 'FDIC/MDRM_test.csv'\n",
    "cap_file = 'FDIC/MDRM_cap_test.csv'\n",
    "\n",
    "pres_df = pd.read_csv(pres_file, delimiter=',')\n",
    "cap_file = pd.read_csv(cap_file, delimiter=',')\n",
    "\n",
    "#test = merge_dataframes([pres_df, cap_file], ['from', 'from'])\n",
    "test = merge_dataframes([(pres_df, 'from'), (cap_file, 'from')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_MDRMdict(input_path =paths.localPath+paths.folder_MDRMs+paths.filename_MDRM_src, export_path =paths.folder_Orig + paths.filename_MDRM, filters=None):\n",
    "    \"\"\"\n",
    "    Reads a CSV file, filters the DataFrame based on the given criteria, \n",
    "    and exports the result to a new CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - input_path: str, path to the input CSV file.\n",
    "    - export_path: str, path where the filtered DataFrame should be saved.\n",
    "    - filters: dict, optional. A dictionary where keys are column names and values \n",
    "               are lists of filter criteria.\n",
    "\n",
    "    Example:\n",
    "    build_MDRMdict(\n",
    "        input_path='path/to/MDRM_CSV.csv',\n",
    "        export_path='path/to/export/MDRM_dict.csv',\n",
    "        filters={'Reporting Form': ['FFIEC 031', 'FFIEC 041', 'FFIEC 051']})\n",
    "    \"\"\"\n",
    "\n",
    "    def format_str_date(date_string):\n",
    "        try:\n",
    "            date_part = date_string.split()[0]\n",
    "            month, day, year = date_part.split('/')\n",
    "            return f\"{year.zfill(4)}-{month.zfill(2)}-{day.zfill(2)}\"\n",
    "        except:\n",
    "            return date_string\n",
    "\n",
    "    # Read the CSV file with headers starting on line 2\n",
    "    df = pd.read_csv(input_path, header=1, sep=',', quotechar='\"') #, parse_dates=['Start Date', 'End Date'])\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "    df[['Start Date', 'End Date']] = df.apply(lambda row: pd.Series([\n",
    "        format_str_date(row['Start Date']),\n",
    "        format_str_date(row['End Date'])]),\n",
    "        axis=1)\n",
    "\n",
    "    # Apply filters if provided\n",
    "    if filters:\n",
    "        for column, values in filters.items():\n",
    "            df = df[df[column].isin(values)]\n",
    "    \n",
    "    #Rename columns and format df to needed format and order\n",
    "    rename_columns={'Start Date':'Start_Date',\n",
    "                    'End Date':'End_Date',\n",
    "                    'Item Name':'Item_Name',\n",
    "                    'Confidentiality':'Confidential',\n",
    "                    'ItemType':'Item Type',\n",
    "                    'Reporting Form':'Reporting_Form',\n",
    "                    'SeriesGlossary':'Series Glossary'}\n",
    "    df.rename(columns=rename_columns, inplace=True)\n",
    "    df['MDRM_Item'] = df['Mnemonic'] + df['Item Code'].astype(str)\n",
    "    df = df[['MDRM_Item', 'Start_Date', 'End_Date', 'Item_Name', 'Confidential', 'Reporting_Form']]\n",
    "\n",
    "    # Export the filtered DataFrame to the specified export path\n",
    "    if export_path:\n",
    "        df.to_csv(export_path, index=False)\n",
    "    else:\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mdf[mdf['End_Date'] > '1999-12-31']\n",
    "#mdf=mdf.drop(columns='Reporting_Form').drop_duplicates()\n",
    "mdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filters = {'Reporting Form':['FFIEC 031', 'FFIEC 041', 'FFIEC 051']}\n",
    "filters = {'Mnemonic':['CALL',\n",
    "                            'CENB','IADX','IBFQ','RCCD','RCCF','RCEG',\n",
    "                       'RCFD',\n",
    "                            'RCFA', 'RCFW',\n",
    "                       'RCFN',\n",
    "                       'RCON',\n",
    "                          'RCF0','RCF1','RCF2','RCF3','RCF4','RCF5','RCF6','RCF7','RCF8','RCF9','RCOA','RCOW',\n",
    "                       'RCOS','RIAD','RIAS','RIDM','RIFN','SCHJ',\n",
    "                  ]}\n",
    "#filters = {'Mnemonic':['CALL']}\n",
    "\n",
    "\n",
    "#build_MDRMdict(export_path =paths.folder_Orig + 'MDRM_test.csv' ,filters=filters)\n",
    "#build_MDRMdict(export_path =paths.folder_Orig + 'MDRM_Dict.csv', filters=filters)\n",
    "mdf =build_MDRMdict(export_path =None, filters=filters)\n",
    "mdf =mdf[mdf['End_Date'] > '1999-12-31']\n",
    "mdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ParseXBRL(filepath):\n",
    "    \"\"\"\n",
    "    Parses Call Reports to extract all MDRM item and value from XBRL file\n",
    "    \"\"\"\n",
    "\n",
    "    def clean_xbrl_tag(text):\n",
    "        remove_text = [\n",
    "                        '{http://www.w3.org/1999/xlink}',\n",
    "                        '{http://www.xbrl.org/2003/linkbase}',\n",
    "                        '{http://www.xbrl.org/2003/instance}',\n",
    "                        '{http://www.ffiec.gov/xbrl/call/concepts}'\n",
    "                    ]\n",
    "           \n",
    "        for rt in remove_text:\n",
    "            text = text.replace(rt, \"\")\n",
    "        return text\n",
    "\n",
    "    xlink = '{http://www.w3.org/1999/xlink}'\n",
    "\n",
    "    # Parse XML\n",
    "    tree  = ET.parse(filepath)\n",
    "    root = tree.getroot()\n",
    "    reporting_form = None\n",
    "\n",
    "    report_values = []\n",
    "    for child in root:\n",
    "        #print(child.tag, child.attrib)\n",
    "        #code = child.tag[child.tag.find('}')+1:]\n",
    "        code = clean_xbrl_tag(child.tag)\n",
    "\n",
    "        if code == 'schemaRef' and reporting_form is None:\n",
    "            #get attrib=href\n",
    "            #parse url for regex='report{0-9}*3'\n",
    "            attrib = child.attrib.get(xlink+'href')\n",
    "            if attrib:\n",
    "                reporting_form = re.search(pattern=r'report\\d{3}', string=attrib)\n",
    "                reporting_form_str = reporting_form.group(0) if reporting_form else None\n",
    "\n",
    "        data_unit = child.attrib.get('unitRef')\n",
    "        data_decimals = child.attrib.get('decimals')\n",
    "        \n",
    "        #Infer datatype based on basic logic and value\n",
    "        if data_decimals:\n",
    "            if int(data_decimals) == 0:\n",
    "                data_type = 'int'\n",
    "            elif int(data_decimals) >0:\n",
    "                data_type = 'float'\n",
    "            else:\n",
    "                raise TypeError()\n",
    "        elif (child.text and child.text.strip().lower() in ['true','false']):\n",
    "            data_type = 'bool'\n",
    "        else:\n",
    "            data_type = 'str'\n",
    "\n",
    "        #NEED SOMETHING MORE ROBUST TO CAPTURE VALUES HERE\n",
    "        if len(code) == 8:\n",
    "            report_values.append([code, child.text, data_type, data_unit, data_decimals, reporting_form_str])\n",
    "\n",
    "    parsed_df = pd.DataFrame(report_values, columns = ['MDRM_Item','Value','Datatype','Unit','Decimals', 'Reporting_Form_CallReport'])\n",
    "    MDRM_df = wsio.ReadCSV(paths.folder_Orig + paths.filename_MDRM)\n",
    "    full_df=parsed_df.set_index('MDRM_Item').join(MDRM_df.set_index('MDRM_Item'))\n",
    "    #print(len(tot[pd.isnull(tot.Item_Name)]))\n",
    "    full_df.reset_index(level=0, inplace=True)\n",
    "    return full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WIP - DATATYPE \n",
    "def build_MDRM_datatype(filepath_MDRM, folderpath_callReport, folderpath_searchString='/*.XBRL', filepath_MDRM_out=paths.folder_Orig+paths.filename_MDRM):\n",
    "\n",
    "    MDRM_df = pd.read_csv(filepath_MDRM, sep=',', quotechar='\"', header=0)\n",
    "    MDRM_df.drop('Reporting Form', axis=1, errors='ignore')\n",
    "\n",
    "    bank_df = pd.DataFrame()    \n",
    "    if folderpath_callReport[-1:] == '/':\n",
    "        folderpath_callReport = folderpath_callReport[:-1]\n",
    "\n",
    "    bank_foldername = folderpath_callReport.rsplit('/',1)[-1]\n",
    "    rssd = bank_foldername[bank_foldername.rfind('_')+1:]\n",
    "\n",
    "\n",
    "    for call_report_filename in glob.glob(folderpath_callReport + folderpath_searchString):\n",
    "        period_date = call_report_filename[call_report_filename.rfind('_')+1:len(call_report_filename)-5]\n",
    "        period_date = datetime.datetime.strptime((call_report_filename[call_report_filename.rfind('_')+1:len(call_report_filename)-5]), '%Y%m%d').date()\n",
    "        #print(period_date, bank_name, rssd)\n",
    "        tb_df = ParseXBRL(call_report_filename)\n",
    "        #print(tdf)\n",
    "        tb_df['ReportPeriodEndDate'] =period_date\n",
    "        tb_df['RSSD_ID'] =rssd\n",
    "        tb_df = tb_df[['MDRM_Item','Datatype','Unit','Decimals', 'ReportPeriodEndDate']] #, 'Reporting_Form']]\n",
    "        try:\n",
    "            #bank_df = pd.concat([bank_df,tb_df])\n",
    "            bank_df = pd.concat([bank_df,tb_df], ignore_index=True).drop_duplicates(['MDRM_Item'], keep='first') #,'Reporting_Form'], keep='first')\n",
    "        except NameError:\n",
    "            bank_df = tb_df\n",
    "    \n",
    "    #Set types of merge item\n",
    "    MDRM_df = MDRM_df.astype(dtype={'MDRM_Item':'string'})\n",
    "    bank_df = bank_df.astype(dtype={'MDRM_Item':'string'})\n",
    "    \n",
    "    #Merge dataframes\n",
    "    merged_MDRM_df = pd.merge(MDRM_df, bank_df, how='right', on='MDRM_Item', left_index=False, right_index=False, suffixes=['_l', '_r']) \n",
    "\n",
    "    if filepath_MDRM_out:\n",
    "        wsio.WriteDataFrame(filepath_MDRM_out, merged_MDRM_df)\n",
    "    else:\n",
    "        return merged_MDRM_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_MDRM = '/Users/kris/Library/CloudStorage/OneDrive-Personal/Documents/Visual Studio 2019/Repos/ffiec-xbrl/PubData/MDRM_Dict.csv'\n",
    "folderpath_bank = '/Users/kris/data_sources/fdic/BulkReports/1ST BANK OF SEA ISLE CITY_148470/'\n",
    "\n",
    "filepath_MDRM_out = '/Users/kris/Library/CloudStorage/OneDrive-Personal/Documents/Visual Studio 2019/Repos/ffiec-xbrl/PubData/MDRM_Dict_2.csv'\n",
    "\n",
    "mdf = build_MDRM_datatype(filepath_MDRM, folderpath_bank, filepath_MDRM_out=None)\n",
    "mdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xdf =mdf.join(fdf, on='MDRM_Item', how='outer', lsuffix='_l', rsuffix='_r')\n",
    "xdf =pd.merge(mdf, fdf, how='outer', on='MDRM_Item', left_index=False, right_index=False, suffixes=['_l', '_r'])\n",
    "xdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '/Users/kris/data_sources/fdic/BulkReports/1ST BANK OF SEA ISLE CITY_148470/1ST BANK OF SEA ISLE CITY_148470_20240331.XBRL'\n",
    "\n",
    "df = ParseXBRL(filepath)\n",
    "#df\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[df['Datatype']=='bool']\n",
    "#df[df['Value']==None]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "attrib = 'http://www.ffiec.gov/xbrl/call/report05/2018-03-31/v161/call-report05-2018-03-31-v161.xsd'\n",
    "\n",
    "report = re.search(pattern=r'report\\d{3}', string=attrib)\n",
    "report_str = report.group(0) if report else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = glob.glob(paths.localPath + paths.folder_BulkReports + '*')\n",
    "folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_str_date(date_string):\n",
    "    try:\n",
    "        date_part = date_string.split()[0]\n",
    "        month, day, year = date_part.split('/')\n",
    "        return f\"{year.zfill(4)}-{month.zfill(2)}-{day.zfill(2)}\"\n",
    "    except:\n",
    "        return date_string\n",
    "            \n",
    "df = pd.read_csv(input_path, header=1, sep=',', quotechar='\"') #, parse_dates=['Start Date', 'End Date'])\n",
    "df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "df[['Start Date', 'End Date']] = df.apply(lambda row: pd.Series([\n",
    "    format_str_date(row['Start Date']),\n",
    "    format_str_date(row['End Date'])]),\n",
    "    axis=1)\n",
    "\n",
    "# Apply filters if provided\n",
    "if filters:\n",
    "    for column, values in filters.items():\n",
    "        df = df[df[column].isin(values)]\n",
    "\n",
    "#Rename columns and format df to needed format and order\n",
    "rename_columns={'Start Date':'Start_Date',\n",
    "                'End Date':'End_Date',\n",
    "                'Item Name':'Item_Name',\n",
    "                'Confidentiality':'Confidential',\n",
    "                'ItemType':'Item Type',\n",
    "                'Reporting Form':'MDRMReport_Form',\n",
    "                'SeriesGlossary':'Series Glossary'}\n",
    "df.rename(columns=rename_columns, inplace=True)\n",
    "df['MDRM_Item'] = (df['Mnemonic'] + df['Item Code'].astype(str)).astype(str)\n",
    "#df = df.astype(dtype={'MDRM_Item':'string'})\n",
    "df = df[['MDRM_Item', 'Start_Date', 'End_Date', 'Item_Name', 'Confidential']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BUILD RSSD_DICT FROM POR - #Clean RSSD_Por Format - Align with and Create RSSD_Dict.csv\n",
    "def Gen_RSSDict_from_PORfiles(folderpath_input=paths.localPath + paths.folder_RSSDs, filepath_output=paths.folder_Orig + paths.filename_RSSD): \n",
    "    \n",
    "    dtypes = {'IDRSSD': np.dtype('str'),\n",
    "          'FDIC Certificate Number': np.dtype('str'),\n",
    "          'OCC Charter Number': np.dtype('str'),\n",
    "          'OTS Docket Number': np.dtype('str'),\n",
    "          'Primary ABA Routing Number': np.dtype('str'),\n",
    "          'Financial Institution Name': np.dtype('str'),\n",
    "          'Financial Institution Address': np.dtype('str'),\n",
    "          'Financial Institution City': np.dtype('str'),\n",
    "          'Financial Institution State': np.dtype('str'),\n",
    "          'Financial Institution Zip Code': np.dtype('str'),\n",
    "          'Financial Institution Filing Type': np.dtype('str'),\n",
    "          'Last Date/Time Submission Updated On': np.dtype('str')}\n",
    "    \n",
    "    f_df = pd.DataFrame()\n",
    "    \n",
    "    #Open File (Bank_dim from RSSD_Por)\n",
    "    for file in glob.glob(folderpath_input + '* POR *.txt'):\n",
    "        t_df = pd.read_csv(file, sep='\\t', index_col=False, quotechar='\"', dtype = dtypes) #, parse_dates=['Last Date/Time Submission Updated On'])\n",
    "\n",
    "\n",
    "        #Clean RSSD_Por File\n",
    "        t_df = t_df.apply(lambda x: x.str.strip() if x.dtype.name == 'object' else x, axis=0)\n",
    "\n",
    "        t_df.rename(columns={'IDRSSD': 'RSSD_ID'}, inplace=True)\n",
    "        t_df.columns = t_df.columns.str.replace(' ', '_')\n",
    "\n",
    "        f_df = pd.concat([t_df, f_df])\n",
    "\n",
    "    if filepath_output:\n",
    "        #Create RSSD_Dict file\n",
    "        f_df.to_csv(filepath_output, sep=',', quotechar='\"', index= False)\n",
    "    else:\n",
    "        return f_df\n",
    "\n",
    "\n",
    "Gen_RSSDict_from_PORfiles(filepath_output=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "path = '/Users/kris/data_sources/fdic/BulkReports/1ST BANK OF SEA ISLE CITY_148470'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "base_path = '/Users/kris/data_sources/fdic/BulkReports/'\n",
    "folder_path = '/Users/kris/data_sources/fdic/BulkReports/1ST BANK OF SEA ISLE CITY_148470/'\n",
    "folder_paths = [folder_path]\n",
    "file_path = '/Users/kris/data_sources/fdic/BulkReports/1ST BANK OF SEA ISLE CITY_148470/1ST BANK OF SEA ISLE CITY_148470_20240331.XBRL'\n",
    "\n",
    "print(os.path.abspath(folder_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = base_path\n",
    "\n",
    "folders = [folder for folder in glob.glob(path + '*') if not(os.path.isfile(folder))]\n",
    "print(folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_paths = []\n",
    "f = folder_paths or folders\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import FDIC.constants as paths\n",
    "\n",
    "MDRM_df = pd.read_csv(paths.folder_Orig + paths.filename_MDRM)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MDRM_Item</th>\n",
       "      <th>Start_Date</th>\n",
       "      <th>End_Date</th>\n",
       "      <th>Item_Name</th>\n",
       "      <th>Confidential</th>\n",
       "      <th>Reporting_Form_Num</th>\n",
       "      <th>Datatype</th>\n",
       "      <th>Unit</th>\n",
       "      <th>Decimals</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1089</th>\n",
       "      <td>RCOA3792</td>\n",
       "      <td>2014-03-31</td>\n",
       "      <td>9999-12-31</td>\n",
       "      <td>TOTAL QUALIFYING CAPITAL ALLOWABLE UNDER THE R...</td>\n",
       "      <td>N</td>\n",
       "      <td>41</td>\n",
       "      <td>int</td>\n",
       "      <td>USD</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2834</th>\n",
       "      <td>RCOA3792</td>\n",
       "      <td>2017-03-31</td>\n",
       "      <td>9999-12-31</td>\n",
       "      <td>TOTAL QUALIFYING CAPITAL ALLOWABLE UNDER THE R...</td>\n",
       "      <td>N</td>\n",
       "      <td>51</td>\n",
       "      <td>int</td>\n",
       "      <td>USD</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     MDRM_Item  Start_Date    End_Date  \\\n",
       "1089  RCOA3792  2014-03-31  9999-12-31   \n",
       "2834  RCOA3792  2017-03-31  9999-12-31   \n",
       "\n",
       "                                              Item_Name Confidential  \\\n",
       "1089  TOTAL QUALIFYING CAPITAL ALLOWABLE UNDER THE R...            N   \n",
       "2834  TOTAL QUALIFYING CAPITAL ALLOWABLE UNDER THE R...            N   \n",
       "\n",
       "      Reporting_Form_Num Datatype Unit  Decimals  \n",
       "1089                  41      int  USD       0.0  \n",
       "2834                  51      int  USD       0.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MDRM_df\n",
    "MDRM_df[MDRM_df['MDRM_Item']=='RCOA3792']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
