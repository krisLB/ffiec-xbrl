{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "path = '/Users/kris/data_sources/fdic/BulkReports/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in glob.glob(path + '*[!.]'):\n",
    "    print(f'{folder}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = glob.glob(path + '*[!.]')\n",
    "print(f'{folders}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "df = pd.DataFrame({'Date': '20240506', 'As_of': datetime.datetime.now()}, index=[0])\n",
    "df0 = pd.DataFrame(columns=['Date', 'As_of'])\n",
    "df1 = pd.DataFrame({'Date': '20240506', 'As_of': datetime.datetime.now()}, index=[0])\n",
    "\n",
    "#df = pd.concat([df, df1], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = pd.concat([df0, df1], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(df1, df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(df['Date'])\n",
    "#df['Date'].isin(['20240506'])\n",
    "\n",
    "'20240506' in df['Date'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipped_dtypes = {'Date': str, 'As_of': datetime.datetime}\n",
    "skipped_dtypes.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "t = time.time()\n",
    "period = 60\n",
    "\n",
    "diff = t - period\n",
    "\n",
    "current_time = time.strftime('%H:%M:%S', time.localtime(t))\n",
    "wait_time = time.strftime('%H:%M:%S', time.localtime(diff))\n",
    "\n",
    "milliseconds_current = int((t - int(t)) * 1000)\n",
    "milliseconds_diff = int((diff - int(diff)) * 1000)\n",
    "\n",
    "print(f'current: {current_time}.{milliseconds_current}\\nwait: {wait_time}.{milliseconds_diff}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "current = time.time()\n",
    "#print(datetime.datetime.fromtimestamp(current + 3600).strftime(\"%Y%m%d %H:%M:%S\"))\n",
    "print(f'{time.strftime(\"%Y%m%d %H:%M:%S\",time.gmtime(current + 3600))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ij = 0\n",
    "for i in range(10):\n",
    "    for j in range(5):\n",
    "        ij +=1\n",
    "print(ij)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to your XBRL file and schema file\n",
    "\n",
    "bulkReport_path = '/Users/kris/data_sources/fdic/BulkReports/'\n",
    "schema_base_path = '/Users/kris/Desktop/BankRegScraper/taxonomy/Call_ 09302023_Form051'\n",
    "#schema_base_path = '/Users/kris/Downloads/_ 09302023_Form051/'\n",
    "\n",
    "xbrl_file_path = bulkReport_path + 'ABINGTON BANK_61476/ABINGTON BANK_61476_20230930.XBRL'\n",
    "schema_file_path = schema_base_path + 'call-report051-2023-09-30-v261.xsd'\n",
    "presentation_file_path = schema_base_path + 'call-report051-2023-09-30-v261-pres.xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from lxml import etree\n",
    "import pandas as pd\n",
    "\n",
    "# Paths to your XBRL file and schema file\n",
    "#xbrl_file_path = '/Users/kris/data_sources/fdic/BulkReports/ABINGTON BANK_61476/ABINGTON BANK_61476_20230930.XBRL'\n",
    "#schema_file_path = '/Users/kris/Downloads/_ 09302023_Form051/call-report051-2023-09-30-v261.xsd'\n",
    "\n",
    "def parse_xsd(schema_path):\n",
    "    with open(schema_path, 'rb') as schema_file:\n",
    "        schema_root = etree.XML(schema_file.read())\n",
    "    return schema_root\n",
    "\n",
    "def parse_xbrl(xbrl_path):\n",
    "    with open(xbrl_path, 'rb') as xbrl_file:\n",
    "        xbrl_root = etree.XML(xbrl_file.read())\n",
    "    return xbrl_root\n",
    "\n",
    "def extract_data_from_xbrl(xbrl_root, schema_root):\n",
    "    namespaces = {'xbrli': 'http://www.xbrl.org/2003/instance'}\n",
    "    context_elements = xbrl_root.xpath('//xbrli:context', namespaces=namespaces)\n",
    "    \n",
    "    data = []\n",
    "    for context in context_elements:\n",
    "        context_id = context.get('id')\n",
    "        data_elements = xbrl_root.xpath(f'//*[@contextRef=\"{context_id}\"]', namespaces=namespaces)\n",
    "        \n",
    "        for element in data_elements:\n",
    "            tag = etree.QName(element).localname\n",
    "            value = element.text\n",
    "            unit = element.get('unitRef', 'N/A')\n",
    "            data.append((context_id, tag, value, unit))\n",
    "    \n",
    "    return data\n",
    "\n",
    "def present_data(data):\n",
    "    df = pd.DataFrame(data, columns=['Context ID', 'Tag', 'Value', 'Unit'])\n",
    "    print(df)\n",
    "\n",
    "def main(xbrl_file_path, schema_file_path):\n",
    "    schema_root = parse_xsd(schema_file_path)\n",
    "    xbrl_root = parse_xbrl(xbrl_file_path)\n",
    "    data = extract_data_from_xbrl(xbrl_root, schema_root)\n",
    "    present_data(data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(xbrl_file_path, schema_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from lxml import etree\n",
    "import pandas as pd\n",
    "\n",
    "# Paths to your XBRL file and schema file\n",
    "#xbrl_file_path = \"path/to/ABINGTON BANK_61476_20230930.XBRL\"\n",
    "#schema_file_path = \"path/to/_09302023_Form051/call-report051-2023-09-30-v261.xsd\"\n",
    "#presentation_file_path = \"path/to/_09302023_Form051/call-report051-2023-09-30-v261-pres.xml\"\n",
    "\n",
    "def parse_xml(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        root = etree.XML(file.read())\n",
    "    return root\n",
    "\n",
    "def validate_xbrl(xbrl_root, schema_root):\n",
    "    schema = etree.XMLSchema(schema_root)\n",
    "    if not schema.validate(xbrl_root):\n",
    "        raise ValueError(\"XBRL file is not valid.\")\n",
    "    return True\n",
    "\n",
    "def extract_data_from_xbrl(xbrl_root):\n",
    "    namespaces = {'xbrli': 'http://www.xbrl.org/2003/instance'}\n",
    "    context_elements = xbrl_root.xpath('//xbrli:context', namespaces=namespaces)\n",
    "    \n",
    "    data = []\n",
    "    for context in context_elements:\n",
    "        context_id = context.get('id')\n",
    "        data_elements = xbrl_root.xpath(f'//*[@contextRef=\"{context_id}\"]', namespaces=namespaces)\n",
    "        \n",
    "        for element in data_elements:\n",
    "            tag = etree.QName(element).localname\n",
    "            value = element.text\n",
    "            unit = element.get('unitRef', 'N/A')\n",
    "            data.append((context_id, tag, value, unit))\n",
    "    \n",
    "    return data\n",
    "\n",
    "def present_data(data):\n",
    "    df = pd.DataFrame(data, columns=['Context ID', 'Tag', 'Value', 'Unit'])\n",
    "    print(df)\n",
    "\n",
    "def main(xbrl_file_path, schema_file_path, presentation_file_path):\n",
    "    schema_root = parse_xml(schema_file_path)\n",
    "    xbrl_root = parse_xml(xbrl_file_path)\n",
    "    #validate_xbrl(xbrl_root, schema_root)\n",
    "    \n",
    "    # Extract data from XBRL instance\n",
    "    data = extract_data_from_xbrl(xbrl_root)\n",
    "    \n",
    "    # Optionally, parse and use the presentation linkbase\n",
    "    presentation_root = parse_xml(presentation_file_path)\n",
    "    \n",
    "    # Present data\n",
    "    present_data(data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(xbrl_file_path, schema_file_path, presentation_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from lxml import etree\n",
    "import pandas as pd\n",
    "\n",
    "# Paths to your XBRL file and schema file\n",
    "#xbrl_file_path = \"path/to/ABINGTON BANK_61476_20230930.XBRL\"\n",
    "#schema_file_path = \"path/to/_09302023_Form051/call-report051-2023-09-30-v261.xsd\"\n",
    "#presentation_file_path = \"path/to/_09302023_Form051/call-report051-2023-09-30-v261-pres.xml\"\n",
    "\n",
    "def parse_xml(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        root = etree.XML(file.read())\n",
    "    return root\n",
    "\n",
    "def validate_xbrl(xbrl_root, schema_root):\n",
    "    schema = etree.XMLSchema(schema_root)\n",
    "    if not schema.validate(xbrl_root):\n",
    "        raise ValueError(\"XBRL file is not valid.\")\n",
    "    return True\n",
    "\n",
    "def extract_data_from_xbrl(xbrl_root):\n",
    "    namespaces = {'xbrli': 'http://www.xbrl.org/2003/instance'}\n",
    "    context_elements = xbrl_root.xpath('//xbrli:context', namespaces=namespaces)\n",
    "    \n",
    "    data = []\n",
    "    for context in context_elements:\n",
    "        context_id = context.get('id')\n",
    "        data_elements = xbrl_root.xpath(f'//*[@contextRef=\"{context_id}\"]', namespaces=namespaces)\n",
    "        \n",
    "        for element in data_elements:\n",
    "            tag = etree.QName(element).localname\n",
    "            value = element.text\n",
    "            unit = element.get('unitRef', 'N/A')\n",
    "            data.append((context_id, tag, value, unit))\n",
    "    \n",
    "    return data\n",
    "\n",
    "def parse_presentation_linkbase(presentation_root):\n",
    "    link_namespace = {'link': 'http://www.xbrl.org/2003/linkbase'}\n",
    "    arcrole = 'http://www.xbrl.org/2003/arcrole/parent-child'\n",
    "    role_refs = presentation_root.xpath('//link:roleRef', namespaces=link_namespace)\n",
    "    \n",
    "    presentations = {}\n",
    "    for role_ref in role_refs:\n",
    "        role_uri = role_ref.get('roleURI')\n",
    "        locators = presentation_root.xpath(f'//link:loc[@xlink:role=\"{role_uri}\"]', namespaces=link_namespace)\n",
    "        arcs = presentation_root.xpath(f'//link:definitionArc[@xlink:arcrole=\"{arcrole}\"]', namespaces=link_namespace)\n",
    "        \n",
    "        elements = []\n",
    "        for arc in arcs:\n",
    "            from_locator = arc.get('from')\n",
    "            to_locator = arc.get('to')\n",
    "            from_element = next((loc for loc in locators if loc.get('label') == from_locator), None)\n",
    "            to_element = next((loc for loc in locators if loc.get('label') == to_locator), None)\n",
    "            if from_element is not None and to_element is not None:\n",
    "                elements.append((from_element.get('{http://www.w3.org/1999/xlink}href'), to_element.get('{http://www.w3.org/1999/xlink}href')))\n",
    "        presentations[role_uri] = elements\n",
    "    \n",
    "    return presentations\n",
    "\n",
    "def present_data(data, presentations):\n",
    "    df = pd.DataFrame(data, columns=['Context ID', 'Tag', 'Value', 'Unit'])\n",
    "    \n",
    "    presentation_order = []\n",
    "    for role_uri, elements in presentations.items():\n",
    "        for parent, child in elements:\n",
    "            presentation_order.append(child.split('#')[-1])\n",
    "    \n",
    "    df['Tag'] = pd.Categorical(df['Tag'], categories=presentation_order, ordered=True)\n",
    "    df.sort_values('Tag', inplace=True)\n",
    "    \n",
    "    print(df)\n",
    "\n",
    "def main(xbrl_file_path, schema_file_path, presentation_file_path):\n",
    "    schema_root = parse_xml(schema_file_path)\n",
    "    xbrl_root = parse_xml(xbrl_file_path)\n",
    "    #validate_xbrl(xbrl_root, schema_root)\n",
    "    \n",
    "    # Extract data from XBRL instance\n",
    "    data = extract_data_from_xbrl(xbrl_root)\n",
    "    \n",
    "    # Parse presentation linkbase\n",
    "    presentation_root = parse_xml(presentation_file_path)\n",
    "    presentations = parse_presentation_linkbase(presentation_root)\n",
    "    \n",
    "    # Present data\n",
    "    present_data(data, presentations)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(xbrl_file_path, schema_file_path, presentation_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from lxml import etree\n",
    "import pandas as pd\n",
    "\n",
    "# Paths to your XBRL file and schema file\n",
    "#xbrl_file_path = \"path/to/ABINGTON BANK_61476_20230930.XBRL\"\n",
    "#schema_file_path = \"path/to/_09302023_Form051/call-report051-2023-09-30-v261.xsd\"\n",
    "#presentation_file_path = \"path/to/_09302023_Form051/call-report051-2023-09-30-v261-pres.xml\"\n",
    "\n",
    "def parse_xml(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        root = etree.XML(file.read())\n",
    "    return root\n",
    "\n",
    "def validate_xbrl(xbrl_root, schema_root):\n",
    "    schema = etree.XMLSchema(schema_root)\n",
    "    if not schema.validate(xbrl_root):\n",
    "        raise ValueError(\"XBRL file is not valid.\")\n",
    "    return True\n",
    "\n",
    "def extract_data_from_xbrl(xbrl_root):\n",
    "    namespaces = {'xbrli': 'http://www.xbrl.org/2003/instance'}\n",
    "    context_elements = xbrl_root.xpath('//xbrli:context', namespaces=namespaces)\n",
    "    \n",
    "    data = []\n",
    "    for context in context_elements:\n",
    "        context_id = context.get('id')\n",
    "        data_elements = xbrl_root.xpath(f'//*[@contextRef=\"{context_id}\"]', namespaces=namespaces)\n",
    "        \n",
    "        for element in data_elements:\n",
    "            tag = etree.QName(element).localname\n",
    "            value = element.text\n",
    "            unit = element.get('unitRef', 'N/A')\n",
    "            data.append((context_id, tag, value, unit))\n",
    "    \n",
    "    return data\n",
    "\n",
    "def parse_presentation_linkbase(presentation_root):\n",
    "    link_namespace = {'link': 'http://www.xbrl.org/2003/linkbase'}\n",
    "    xlink_namespace = {'xlink': 'http://www.w3.org/1999/xlink'}\n",
    "    \n",
    "    locators = {}\n",
    "    for loc in presentation_root.xpath('//link:loc', namespaces=link_namespace):\n",
    "        label = loc.get('{http://www.w3.org/1999/xlink}label')\n",
    "        href = loc.get('{http://www.w3.org/1999/xlink}href')\n",
    "        locators[label] = href.split('#')[-1]\n",
    "    \n",
    "    presentation_order = []\n",
    "    for arc in presentation_root.xpath('//link:presentationArc', namespaces=link_namespace):\n",
    "        from_label = arc.get('from')\n",
    "        to_label = arc.get('to')\n",
    "        if from_label in locators and to_label in locators:\n",
    "            presentation_order.append(locators[to_label])\n",
    "    \n",
    "    return presentation_order\n",
    "\n",
    "def present_data(data, presentation_order):\n",
    "    df = pd.DataFrame(data, columns=['Context ID', 'Tag', 'Value', 'Unit'])\n",
    "    \n",
    "    # Use presentation order to categorize and sort tags\n",
    "    df['Tag'] = pd.Categorical(df['Tag'], categories=presentation_order, ordered=True)\n",
    "    df.sort_values('Tag', inplace=True)\n",
    "    \n",
    "    print(df)\n",
    "\n",
    "def main(xbrl_file_path, schema_file_path, presentation_file_path):\n",
    "    schema_root = parse_xml(schema_file_path)\n",
    "    xbrl_root = parse_xml(xbrl_file_path)\n",
    "    #validate_xbrl(xbrl_root, schema_root)\n",
    "    \n",
    "    # Extract data from XBRL instance\n",
    "    data = extract_data_from_xbrl(xbrl_root)\n",
    "    \n",
    "    # Parse presentation linkbase\n",
    "    presentation_root = parse_xml(presentation_file_path)\n",
    "    presentation_order = parse_presentation_linkbase(presentation_root)\n",
    "    \n",
    "    # Present data\n",
    "    present_data(data, presentation_order)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(xbrl_file_path, schema_file_path, presentation_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from lxml import etree\n",
    "import pandas as pd\n",
    "\n",
    "# Paths to your XBRL file and schema file\n",
    "#xbrl_file_path = \"/path/to/ABINGTON BANK_61476_20230930.XBRL\"\n",
    "#schema_file_path = \"/path/to/_09302023_Form051/call-report051-2023-09-30-v261.xsd\"\n",
    "#presentation_file_path = \"/mnt/data/call-report051-2023-09-30-v261-pres.xml\"\n",
    "\n",
    "def parse_xml(file_path):\n",
    "    print(f\"Parsing XML file: {file_path}\")\n",
    "    with open(file_path, 'rb') as file:\n",
    "        root = etree.XML(file.read())\n",
    "    return root\n",
    "\n",
    "def validate_xbrl(xbrl_root, schema_root):\n",
    "    print(\"Validating XBRL file\")\n",
    "    schema = etree.XMLSchema(schema_root)\n",
    "    if not schema.validate(xbrl_root):\n",
    "        raise ValueError(\"XBRL file is not valid.\")\n",
    "    return True\n",
    "\n",
    "def extract_data_from_xbrl(xbrl_root):\n",
    "    print(\"Extracting data from XBRL\")\n",
    "    namespaces = {'xbrli': 'http://www.xbrl.org/2003/instance'}\n",
    "    context_elements = xbrl_root.xpath('//xbrli:context', namespaces=namespaces)\n",
    "    \n",
    "    data = []\n",
    "    for context in context_elements:\n",
    "        context_id = context.get('id')\n",
    "        data_elements = xbrl_root.xpath(f'//*[@contextRef=\"{context_id}\"]', namespaces=namespaces)\n",
    "        \n",
    "        for element in data_elements:\n",
    "            tag = etree.QName(element).localname\n",
    "            value = element.text\n",
    "            unit = element.get('unitRef', 'N/A')\n",
    "            data.append((context_id, tag, value, unit))\n",
    "    \n",
    "    return data\n",
    "\n",
    "def parse_presentation_linkbase(presentation_root):\n",
    "    print(\"Parsing presentation linkbase\")\n",
    "    link_namespace = {'link': 'http://www.xbrl.org/2003/linkbase'}\n",
    "    xlink_namespace = {'xlink': 'http://www.w3.org/1999/xlink'}\n",
    "    \n",
    "    locators = {}\n",
    "    for loc in presentation_root.xpath('//link:loc', namespaces=link_namespace):\n",
    "        label = loc.get('{http://www.w3.org/1999/xlink}label')\n",
    "        href = loc.get('{http://www.w3.org/1999/xlink}href')\n",
    "        locators[label] = href.split('#')[-1]\n",
    "    \n",
    "    presentation_order = []\n",
    "    hierarchy = {}\n",
    "    for arc in presentation_root.xpath('//link:presentationArc', namespaces=link_namespace):\n",
    "        from_label = arc.get('from')\n",
    "        to_label = arc.get('to')\n",
    "        if from_label in locators and to_label in locators:\n",
    "            presentation_order.append(locators[to_label])\n",
    "            if locators[from_label] not in hierarchy:\n",
    "                hierarchy[locators[from_label]] = []\n",
    "            hierarchy[locators[from_label]].append(locators[to_label])\n",
    "    \n",
    "    return presentation_order, hierarchy\n",
    "\n",
    "def present_data(data, presentation_order, hierarchy):\n",
    "    print(\"Presenting data\")\n",
    "    df = pd.DataFrame(data, columns=['Context ID', 'Tag', 'Value', 'Unit'])\n",
    "    \n",
    "    # Use presentation order to categorize and sort tags\n",
    "    df['Tag'] = pd.Categorical(df['Tag'], categories=presentation_order, ordered=True)\n",
    "    df.sort_values('Tag', inplace=True)\n",
    "    \n",
    "    # Display the hierarchical structure\n",
    "    for parent, children in hierarchy.items():\n",
    "        print(f\"{parent}:\")\n",
    "        for child in children:\n",
    "            print(f\"  - {child}\")\n",
    "    \n",
    "    print(df)\n",
    "\n",
    "def main(xbrl_file_path, schema_file_path, presentation_file_path):\n",
    "    print(\"Starting main process\")\n",
    "    schema_root = parse_xml(schema_file_path)\n",
    "    xbrl_root = parse_xml(xbrl_file_path)\n",
    "    #validate_xbrl(xbrl_root, schema_root)\n",
    "    \n",
    "    # Extract data from XBRL instance\n",
    "    data = extract_data_from_xbrl(xbrl_root)\n",
    "    \n",
    "    # Parse presentation linkbase\n",
    "    presentation_root = parse_xml(presentation_file_path)\n",
    "    presentation_order, hierarchy = parse_presentation_linkbase(presentation_root)\n",
    "    \n",
    "    # Present data\n",
    "    present_data(data, presentation_order, hierarchy)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(xbrl_file_path, schema_file_path, presentation_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from lxml import etree\n",
    "import pandas as pd\n",
    "\n",
    "# Paths to your XBRL file, schema file, and taxonomy files\n",
    "bulkReport_path = '/Users/kris/data_sources/fdic/BulkReports/'\n",
    "schema_path = '/Users/kris/Desktop/BankRegScraper/taxonomy/Call_ 12312023_Form051/'\n",
    "\n",
    "xbrl_file_path = os.path.join(bulkReport_path, \"ABINGTON BANK_61476/ABINGTON BANK_61476_20231231.XBRL\")\n",
    "schema_files = [\n",
    "    os.path.join(schema_path, \"call-report051-2023-12-31-v265.xsd\"),\n",
    "    os.path.join(schema_path, \"concepts.xsd\"),\n",
    "    os.path.join(schema_path, \"ffiec-instance-2004-06-10.xsd\"),\n",
    "    os.path.join(schema_path, \"ffiec-linkbase-2004-05-14.xsd\"),\n",
    "    os.path.join(schema_path, \"mdr_schema.xsd\")\n",
    "]\n",
    "linkbase_files = {\n",
    "    \"presentation\": os.path.join(schema_path, \"call-report051-2023-12-31-v265-pres.xml\"),\n",
    "    \"calculation\": os.path.join(schema_path, \"call-report051-2023-12-31-v265-calc.xml\"),\n",
    "    \"definition\": os.path.join(schema_path, \"call-report051-2023-12-31-v265-def.xml\"),\n",
    "    \"reference\": os.path.join(schema_path, \"call-report051-2023-12-31-v265-ref.xml\"),\n",
    "    \"label\": os.path.join(schema_path, \"call-report051-2023-12-31-v265-instr.xml\"),\n",
    "    \"ec-mess\": os.path.join(schema_path, \"call-report051-2023-12-31-v265-ec-mess.xml\"),\n",
    "    \"ec\": os.path.join(schema_path, \"call-report051-2023-12-31-v265-ec.xml\")\n",
    "}\n",
    "\n",
    "def parse_xml(file_path):\n",
    "    print(f\"Parsing XML file: {file_path}\")\n",
    "    try:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            root = etree.XML(file.read())\n",
    "        return root\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to parse XML file: {file_path}. Error: {e}\")\n",
    "        raise\n",
    "\n",
    "def validate_xbrl(xbrl_root, schema_roots):\n",
    "    print(\"Validating XBRL file\")\n",
    "    for schema_root in schema_roots:\n",
    "        print(f\"Validating against schema: {schema_root}\")\n",
    "        schema = etree.XMLSchema(schema_root)\n",
    "        if not schema.validate(xbrl_root):\n",
    "            print(f\"Validation failed against schema: {schema_root}\")\n",
    "            raise ValueError(\"XBRL file is not valid against the schema.\")\n",
    "        else:\n",
    "            print(f\"Validation succeeded against schema: {schema_root}\")\n",
    "    print(\"Validation completed\")\n",
    "    return True\n",
    "\n",
    "def extract_data_from_xbrl(xbrl_root):\n",
    "    print(\"Extracting data from XBRL\")\n",
    "    namespaces = {'xbrli': 'http://www.xbrl.org/2003/instance'}\n",
    "    context_elements = xbrl_root.xpath('//xbrli:context', namespaces=namespaces)\n",
    "    \n",
    "    data = []\n",
    "    for context in context_elements:\n",
    "        context_id = context.get('id')\n",
    "        data_elements = xbrl_root.xpath(f'//*[@contextRef=\"{context_id}\"]', namespaces=namespaces)\n",
    "        \n",
    "        for element in data_elements:\n",
    "            tag = etree.QName(element).localname\n",
    "            value = element.text\n",
    "            unit = element.get('unitRef', 'N/A')\n",
    "            data.append((context_id, tag, value, unit))\n",
    "    \n",
    "    return data\n",
    "\n",
    "def parse_linkbase(linkbase_file, arc_tag):\n",
    "    print(f\"Parsing {arc_tag} linkbase\")\n",
    "    link_namespace = {'link': 'http://www.xbrl.org/2003/linkbase'}\n",
    "    \n",
    "    root = parse_xml(linkbase_file)\n",
    "    locators = {}\n",
    "    for loc in root.xpath('//link:loc', namespaces=link_namespace):\n",
    "        label = loc.get('{http://www.w3.org/1999/xlink}label')\n",
    "        href = loc.get('{http://www.w3.org/1999/xlink}href')\n",
    "        locators[label] = href.split('#')[-1]\n",
    "    \n",
    "    arcs = []\n",
    "    for arc in root.xpath(f'//link:{arc_tag}Arc', namespaces=link_namespace):\n",
    "        from_label = arc.get('from')\n",
    "        to_label = arc.get('to')\n",
    "        if from_label in locators and to_label in locators:\n",
    "            arcs.append((locators[from_label], locators[to_label]))\n",
    "    \n",
    "    return locators, arcs\n",
    "\n",
    "def parse_presentation_linkbase(presentation_file):\n",
    "    locators, presentation_arcs = parse_linkbase(presentation_file, 'presentation')\n",
    "    presentation_order = []\n",
    "    hierarchy = {}\n",
    "    for parent, child in presentation_arcs:\n",
    "        presentation_order.append(child)\n",
    "        if parent not in hierarchy:\n",
    "            hierarchy[parent] = []\n",
    "        hierarchy[parent].append(child)\n",
    "    \n",
    "    return presentation_order, hierarchy\n",
    "\n",
    "def parse_calculation_linkbase(calculation_file):\n",
    "    locators, calculation_arcs = parse_linkbase(calculation_file, 'calculation')\n",
    "    calculations = {}\n",
    "    for parent, child in calculation_arcs:\n",
    "        if parent not in calculations:\n",
    "            calculations[parent] = []\n",
    "        calculations[parent].append(child)\n",
    "    \n",
    "    return calculations\n",
    "\n",
    "# Function to parse the definition linkbase with additional debug statements\n",
    "def parse_definition_linkbase(definition_file):\n",
    "    print(f\"Parsing definition linkbase: {definition_file}\")\n",
    "    link_namespace = {'link': 'http://www.xbrl.org/2003/linkbase'}\n",
    "    \n",
    "    try:\n",
    "        root = etree.parse(definition_file)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to parse definition linkbase file: {definition_file}. Error: {e}\")\n",
    "        return {}\n",
    "    \n",
    "    locators = {}\n",
    "    for loc in root.xpath('//link:loc', namespaces=link_namespace):\n",
    "        label = loc.get('{http://www.w3.org/1999/xlink}label')\n",
    "        href = loc.get('{http://www.w3.org/1999/xlink}href')\n",
    "        locators[label] = href.split('#')[-1]\n",
    "    \n",
    "    print(f\"Locators found: {locators}\")\n",
    "    \n",
    "    definition_arcs = []\n",
    "    for arc in root.xpath('//link:defArc', namespaces=link_namespace):\n",
    "        from_label = arc.get('from')\n",
    "        to_label = arc.get('to')\n",
    "        if from_label in locators and to_label in locators:\n",
    "            definition_arcs.append((locators[from_label], locators[to_label]))\n",
    "    \n",
    "    print(f\"Definition arcs found: {definition_arcs}\")\n",
    "    \n",
    "    definitions = {}\n",
    "    for parent, child in definition_arcs:\n",
    "        if parent not in definitions:\n",
    "            definitions[parent] = []\n",
    "        definitions[parent].append(child)\n",
    "    \n",
    "    print(f\"Definitions: {definitions}\")\n",
    "    return definitions\n",
    "    \n",
    "def parse_additional_linkbases(additional_file):\n",
    "    # Add logic to parse additional linkbases if needed\n",
    "    pass\n",
    "\n",
    "def present_data(data, presentation_order, hierarchy, calculations, definitions):\n",
    "    print(\"Presenting data\")\n",
    "    df = pd.DataFrame(data, columns=['Context ID', 'Tag', 'Value', 'Unit'])\n",
    "    \n",
    "    # Use presentation order to categorize and sort tags\n",
    "    df['Tag'] = pd.Categorical(df['Tag'], categories=presentation_order, ordered=True)\n",
    "    df.sort_values('Tag', inplace=True)\n",
    "    \n",
    "    # Display the hierarchical structure\n",
    "    print(\"Hierarchy:\")\n",
    "    for parent, children in hierarchy.items():\n",
    "        print(f\"{parent}:\")\n",
    "        for child in children:\n",
    "            print(f\"  - {child}\")\n",
    "    \n",
    "    # Display calculation relationships\n",
    "    print(\"\\nCalculations:\")\n",
    "    for parent, children in calculations.items():\n",
    "        print(f\"{parent}:\")\n",
    "        for child in children:\n",
    "            print(f\"  - {child}\")\n",
    "    \n",
    "    # Display definition relationships\n",
    "    print(\"\\nDefinitions:\")\n",
    "    for parent, children in definitions.items():\n",
    "        print(f\"{parent}:\")\n",
    "        for child in children:\n",
    "            print(f\"  - {child}\")\n",
    "    \n",
    "    print(\"\\nData:\")\n",
    "    print(df)\n",
    "\n",
    "def main(xbrl_file_path, schema_files, linkbase_files):\n",
    "    print(\"Starting main process\")\n",
    "    schema_roots = [parse_xml(file_path) for file_path in schema_files]\n",
    "    xbrl_root = parse_xml(xbrl_file_path)\n",
    "    \n",
    "    #validate_xbrl(xbrl_root, schema_roots)\n",
    "    \n",
    "    # Extract data from XBRL instance\n",
    "    data = extract_data_from_xbrl(xbrl_root)\n",
    "    \n",
    "    # Parse linkbases\n",
    "    presentation_order, hierarchy = parse_presentation_linkbase(linkbase_files['presentation'])\n",
    "    calculations = parse_calculation_linkbase(linkbase_files['calculation'])\n",
    "    definitions = parse_definition_linkbase(linkbase_files['definition'])\n",
    "    \n",
    "    # Parse additional linkbases if needed\n",
    "    parse_additional_linkbases(linkbase_files['ec-mess'])\n",
    "    parse_additional_linkbases(linkbase_files['ec'])\n",
    "    \n",
    "    # Present data\n",
    "    present_data(data, presentation_order, hierarchy, calculations, definitions)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(xbrl_file_path, schema_files, linkbase_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from lxml import etree\n",
    "import pandas as pd\n",
    "\n",
    "# Paths to your XBRL file, schema file, and taxonomy files\n",
    "bulkReport_path = '/Users/kris/data_sources/fdic/BulkReports/'\n",
    "schema_path = '/Users/kris/Desktop/BankRegScraper/taxonomy/Call_ 12312023_Form051/'\n",
    "\n",
    "xbrl_file_path = os.path.join(bulkReport_path, \"ABINGTON BANK_61476/ABINGTON BANK_61476_20230930.XBRL\")\n",
    "schema_files = [\n",
    "    os.path.join(schema_path, \"call-report051-2023-12-31-v265.xsd\"),\n",
    "    os.path.join(schema_path, \"concepts.xsd\"),\n",
    "    os.path.join(schema_path, \"ffiec-instance-2004-06-10.xsd\"),\n",
    "    os.path.join(schema_path, \"ffiec-linkbase-2004-05-14.xsd\"),\n",
    "    os.path.join(schema_path, \"mdr_schema.xsd\")\n",
    "]\n",
    "linkbase_files = {\n",
    "    \"presentation\": os.path.join(schema_path, \"call-report051-2023-12-31-v265-pres.xml\"),\n",
    "    \"calculation\": os.path.join(schema_path, \"call-report051-2023-12-31-v265-calc.xml\"),\n",
    "    \"definition\": os.path.join(schema_path, \"call-report051-2023-12-31-v265-def.xml\"),\n",
    "    \"reference\": os.path.join(schema_path, \"call-report051-2023-12-31-v265-ref.xml\"),\n",
    "    \"label\": os.path.join(schema_path, \"call-report051-2023-12-31-v265-instr.xml\"),\n",
    "    \"ec-mess\": os.path.join(schema_path, \"call-report051-2023-12-31-v265-ec-mess.xml\"),\n",
    "    \"ec\": os.path.join(schema_path, \"call-report051-2023-12-31-v265-ec.xml\")\n",
    "}\n",
    "\n",
    "def parse_xml(file_path):\n",
    "    print(f\"Parsing XML file: {file_path}\")\n",
    "    try:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            root = etree.XML(file.read())\n",
    "        return root\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to parse XML file: {file_path}. Error: {e}\")\n",
    "        raise\n",
    "\n",
    "def validate_xbrl(xbrl_root, schema_roots):\n",
    "    print(\"Validating XBRL file\")\n",
    "    for schema_root in schema_roots:\n",
    "        print(f\"Validating against schema: {schema_root}\")\n",
    "        schema = etree.XMLSchema(schema_root)\n",
    "        if not schema.validate(xbrl_root):\n",
    "            print(f\"Validation failed against schema: {schema_root}\")\n",
    "            raise ValueError(\"XBRL file is not valid against the schema.\")\n",
    "        else:\n",
    "            print(f\"Validation succeeded against schema: {schema_root}\")\n",
    "    print(\"Validation completed\")\n",
    "    return True\n",
    "\n",
    "def extract_data_from_xbrl(xbrl_root):\n",
    "    print(\"Extracting data from XBRL\")\n",
    "    namespaces = {'xbrli': 'http://www.xbrl.org/2003/instance'}\n",
    "    context_elements = xbrl_root.xpath('//xbrli:context', namespaces=namespaces)\n",
    "    \n",
    "    data = []\n",
    "    for context in context_elements:\n",
    "        context_id = context.get('id')\n",
    "        data_elements = xbrl_root.xpath(f'//*[@contextRef=\"{context_id}\"]', namespaces=namespaces)\n",
    "        \n",
    "        for element in data_elements:\n",
    "            tag = etree.QName(element).localname\n",
    "            value = element.text\n",
    "            unit = element.get('unitRef', 'N/A')\n",
    "            data.append((context_id, tag, value, unit))\n",
    "    \n",
    "    return data\n",
    "\n",
    "def parse_linkbase(linkbase_file, arc_tag):\n",
    "    print(f\"Parsing {arc_tag} linkbase: {linkbase_file}\")\n",
    "    link_namespace = {'link': 'http://www.xbrl.org/2003/linkbase'}\n",
    "    \n",
    "    root = parse_xml(linkbase_file)\n",
    "    locators = {}\n",
    "    for loc in root.xpath('//link:loc', namespaces=link_namespace):\n",
    "        label = loc.get('{http://www.w3.org/1999/xlink}label')\n",
    "        href = loc.get('{http://www.w3.org/1999/xlink}href')\n",
    "        locators[label] = href.split('#')[-1]\n",
    "    \n",
    "    print(f\"Locators found: {locators}\")\n",
    "    \n",
    "    arcs = []\n",
    "    for arc in root.xpath(f'//link:{arc_tag}Arc', namespaces=link_namespace):\n",
    "        from_label = arc.get('from')\n",
    "        to_label = arc.get('to')\n",
    "        #print(f\"Processing {arc_tag}Arc from {from_label} to {to_label}\")\n",
    "        if from_label in locators and to_label in locators:\n",
    "            arcs.append((locators[from_label], locators[to_label]))\n",
    "    \n",
    "    print(f\"Arcs found in {arc_tag} linkbase: {arcs}\")\n",
    "    return locators, arcs\n",
    "\n",
    "def parse_presentation_linkbase(presentation_file):\n",
    "    locators, presentation_arcs = parse_linkbase(presentation_file, 'presentation')\n",
    "    presentation_order = []\n",
    "    hierarchy = {}\n",
    "    for parent, child in presentation_arcs:\n",
    "        presentation_order.append(child)\n",
    "        if parent not in hierarchy:\n",
    "            hierarchy[parent] = []\n",
    "        hierarchy[parent].append(child)\n",
    "    \n",
    "    return presentation_order, hierarchy\n",
    "\n",
    "def parse_calculation_linkbase(calculation_file):\n",
    "    print(f\"Parsing calculation linkbase: {calculation_file}\")\n",
    "    locators, calculation_arcs = parse_linkbase(calculation_file, 'calculation')\n",
    "    calculations = {}\n",
    "    for parent, child in calculation_arcs:\n",
    "        if parent not in calculations:\n",
    "            calculations[parent] = []\n",
    "        calculations[parent].append(child)\n",
    "    \n",
    "    print(f\"Calculations: {calculations}\")\n",
    "    return calculations\n",
    "\n",
    "def parse_definition_linkbase(definition_file):\n",
    "    print(f\"Parsing definition linkbase: {definition_file}\")\n",
    "    link_namespace = {'link': 'http://www.xbrl.org/2003/linkbase'}\n",
    "    \n",
    "    root = parse_xml(definition_file)\n",
    "    locators = {}\n",
    "    for loc in root.xpath('//link:loc', namespaces=link_namespace):\n",
    "        label = loc.get('{http://www.w3.org/1999/xlink}label')\n",
    "        href = loc.get('{http://www.w3.org/1999/xlink}href')\n",
    "        locators[label] = href.split('#')[-1]\n",
    "    \n",
    "    print(f\"Locators found: {locators}\")\n",
    "    \n",
    "    definition_arcs = []\n",
    "    for arc in root.xpath('//link:defArc', namespaces=link_namespace):\n",
    "        from_label = arc.get('from')\n",
    "        to_label = arc.get('to')\n",
    "        print(f\"Processing defArc from {from_label} to {to_label}\")\n",
    "        if from_label in locators and to_label in locators:\n",
    "            definition_arcs.append((locators[from_label], locators[to_label]))\n",
    "    \n",
    "    print(f\"Definition arcs found: {definition_arcs}\")\n",
    "    \n",
    "    definitions = {}\n",
    "    for parent, child in definition_arcs:\n",
    "        if parent not in definitions:\n",
    "            definitions[parent] = []\n",
    "        definitions[parent].append(child)\n",
    "    \n",
    "    print(f\"Definitions: {definitions}\")\n",
    "    return definitions\n",
    "\n",
    "def parse_additional_linkbases(additional_file):\n",
    "    # Add logic to parse additional linkbases if needed\n",
    "    pass\n",
    "\n",
    "def present_data(data, presentation_order, hierarchy, calculations=None, definitions=None):\n",
    "    print(\"Presenting data\")\n",
    "    df = pd.DataFrame(data, columns=['Context ID', 'Tag', 'Value', 'Unit'])\n",
    "    \n",
    "    # Use presentation order to categorize and sort tags\n",
    "    df['Tag'] = pd.Categorical(df['Tag'], categories=presentation_order, ordered=True)\n",
    "    df.sort_values('Tag', inplace=True)\n",
    "    \n",
    "    # Display the hierarchical structure\n",
    "    print(\"Hierarchy:\")\n",
    "    for parent, children in hierarchy.items():\n",
    "        print(f\"{parent}: {children}\")\n",
    "    \n",
    "    # Display calculations\n",
    "    if calculations:\n",
    "        print(\"\\nCalculations:\")\n",
    "        for parent, children in calculations.items():\n",
    "            print(f\"{parent}: {children}\")\n",
    "    \n",
    "    # Display definitions\n",
    "    if definitions:\n",
    "        print(\"\\nDefinitions:\")\n",
    "        for parent, children in definitions.items():\n",
    "            print(f\"{parent}: {children}\")\n",
    "\n",
    "def main():\n",
    "    # Parsing the provided definition linkbase\n",
    "    definitions = parse_definition_linkbase(linkbase_files['definition'])\n",
    "\n",
    "    # Parsing other necessary linkbases\n",
    "    calculations = parse_calculation_linkbase(linkbase_files['calculation'])\n",
    "    presentation_order, hierarchy = parse_presentation_linkbase(linkbase_files['presentation'])\n",
    "\n",
    "    # Parsing and validating XBRL data\n",
    "    xbrl_root = parse_xml(xbrl_file_path)\n",
    "    schema_roots = [parse_xml(schema_file) for schema_file in schema_files]\n",
    "    #validate_xbrl(xbrl_root, schema_roots)\n",
    "\n",
    "    # Extracting data from XBRL\n",
    "    data = extract_data_from_xbrl(xbrl_root)\n",
    "\n",
    "    # Presenting data\n",
    "    present_data(data, presentation_order, hierarchy, calculations=calculations, definitions=definitions)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.exceptions import ReadTimeout\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "class TimeoutHTTPAdapter(HTTPAdapter):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.timeout = kwargs.pop(\"timeout\", None)\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def send(self, request, **kwargs):\n",
    "        timeout = kwargs.get(\"timeout\")\n",
    "        if timeout is None:\n",
    "            kwargs[\"timeout\"] = self.timeout\n",
    "        \n",
    "        try:\n",
    "            response = super().send(request, **kwargs)\n",
    "        except ReadTimeout as e:\n",
    "            print(f\"ReadTimeout occurred: {e}\")\n",
    "            # Handle the ReadTimeout error here as needed\n",
    "            raise\n",
    "        return response\n",
    "\n",
    "# Usage\n",
    "def get_session_with_timeout(timeout):\n",
    "    session = requests.Session()\n",
    "    \n",
    "    # Define retries\n",
    "    retries = Retry(total=3, backoff_factor=0.3, status_forcelist=[500, 502, 503, 504])\n",
    "    \n",
    "    # Create an instance of TimeoutHTTPAdapter with a specific timeout\n",
    "    adapter = TimeoutHTTPAdapter(timeout=timeout, max_retries=retries)\n",
    "    \n",
    "    # Mount the adapter to the session for both HTTP and HTTPS\n",
    "    session.mount(\"http://\", adapter)\n",
    "    session.mount(\"https://\", adapter)\n",
    "    \n",
    "    return session\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    session = get_session_with_timeout(timeout=5)  # 5 seconds timeout\n",
    "    \n",
    "    try:\n",
    "        response = session.get(\"https://httpbin.org/delay/10\")  # This should trigger a timeout\n",
    "        print(response.status_code)\n",
    "    except ReadTimeout:\n",
    "        print(\"A read timeout occurred.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {Exception.__name__}:: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zeep import Client \n",
    "from zeep.wsse.username import UsernameToken\n",
    "\n",
    "\n",
    "client = Client(wsdl, wsse=UsernameToken(ffiec_un,ffiec_pw))\n",
    "service = ZeepServiceProxy(client.service, rate_limiter)\n",
    "\n",
    "response = service.RetrieveFilersSubmissionDateTime(dataSeries = 'Call',\n",
    "                                                            reportingPeriodEndDate = finlPeriod,\n",
    "                                                            lastUpdateDateTime = lastUpdate_finlPeriod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import FDIC.constants as paths\n",
    "import os.path\n",
    "import glob\n",
    "\n",
    "#path = paths.localPath + paths.folder_BulkReports\n",
    "path = '/Users/kris/data_sources/fdic/BulkReports/ABBEVILLE BUILDING AND LOAN A STATE CHARTERED SAVINGS BANK_114774'\n",
    "\n",
    "def get_latest_mod_date(path, file_search_pattern='*'):\n",
    "    \"\"\"\n",
    "    Get the latest modification date of files in a directory.\n",
    "    \"\"\"\n",
    "    latest_date = None\n",
    "    for file_path in glob.glob(path + file_search_pattern):\n",
    "        file_date = os.path.getmtime(file_path)\n",
    "        if latest_date is None or file_date > latest_date:\n",
    "            latest_date = file_date\n",
    "    return latest_date\n",
    "\n",
    "\n",
    "#if get_latest_mod_date(path=path, file_search_pattern='*_master.csv') > get_latest_mod_date(path=path, file_search_pattern='/*.XBRL')\n",
    "\n",
    "date_master = get_latest_mod_date(path=path, file_search_pattern='/*_master.csv')\n",
    "date_xbrl = get_latest_mod_date(path=path, file_search_pattern='/*.XBRL')\n",
    "\n",
    "print(date_master,' || ', date_xbrl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import FDIC.wsio as wsio\n",
    "\n",
    "\n",
    "filepath = '/Users/kris/data_sources/fdic/Src/Taxonomies/Call_ 12312023_Form051/call-report051-2023-12-31-v265-cap.xml'\n",
    "\n",
    "def ParseXBRL(self, filepath):\n",
    "    \"\"\"\n",
    "    Parses Call Reports to extract all MDRM item and value from XBRL file\n",
    "    \"\"\"\n",
    "    # Parse XML\n",
    "    tree  = ET.parse(filepath)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    report_values = []\n",
    "    for child in root:\n",
    "        #print(child.tag, child.attrib)\n",
    "        code = child.tag[child.tag.find('}')+1:]\n",
    "        if len(code) == 8:\n",
    "            report_values.append([code, child.text])\n",
    "\n",
    "    parsed_df = pd.DataFrame(report_values, columns = ['MDRM_Item','Value'])\n",
    "    MDRM_df = wsio.ReadCSV(paths.folder_Orig + paths.filename_MDRM)\n",
    "    full_df=parsed_df.set_index('MDRM_Item').join(MDRM_df.set_index('MDRM_Item'))\n",
    "    #print(len(tot[pd.isnull(tot.Item_Name)]))\n",
    "    full_df.reset_index(level=0, inplace=True)\n",
    "    return full_df\n",
    "\n",
    "\n",
    "tree  = ET.parse(filepath)\n",
    "root = tree.getroot()\n",
    "\n",
    "report_values = []\n",
    "for child in root:\n",
    "    #print(child.tag, child.attrib)\n",
    "    code = child.tag[child.tag.find('}')+1:]\n",
    "    if len(code) == 8:\n",
    "        report_values.append([code, child.text])\n",
    "\n",
    "parsed_df = pd.DataFrame(report_values, columns = ['MDRM_Item','Value'])\n",
    "MDRM_df = wsio.ReadCSV(paths.folder_Orig + paths.filename_MDRM)\n",
    "full_df=parsed_df.set_index('MDRM_Item').join(MDRM_df.set_index('MDRM_Item'))\n",
    "#print(len(tot[pd.isnull(tot.Item_Name)]))\n",
    "full_df.reset_index(level=0, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pres_file_path = '/Users/kris/data_sources/fdic/Src/Taxonomies/Call_ 12312023_Form051/call-report051-2023-12-31-v265-pres.xml'\n",
    "cap_file_path = '/Users/kris/data_sources/fdic/Src/Taxonomies/Call_ 12312023_Form051/call-report051-2023-12-31-v265-cap.xml'\n",
    "\n",
    "path = '/Users/kris/data_sources/fdic/Src/Taxonomies/Call_ 12312023_Form051/call-report051-2023-12-31-v265-pres.xml'\n",
    "pres_search_string = '/*pres.xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_text(input_data):\n",
    "    texts_to_remove = [\n",
    "        '{http://www.w3.org/1999/xlink}',\n",
    "        '{http://www.xbrl.org/2003/linkbase}'\n",
    "        ]\n",
    "    \n",
    "    def removeText(text, texts_to_remove):\n",
    "        for string in texts_to_remove:\n",
    "            text = text.replace(string, \"\")\n",
    "        return text\n",
    "\n",
    "    if isinstance(input_data, str):\n",
    "        return removeText(input_data, texts_to_remove)\n",
    "    elif isinstance(input_data, dict):\n",
    "        return {removeText(key, texts_to_remove) if isinstance(key, str) else key: removeText(value, texts_to_remove) if isinstance(value, str) else value\n",
    "                for key, value in input_data.items()}\n",
    "    else:\n",
    "        raise TypeError(\"Input data must be either a str or a dict\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import FDIC.constants as paths\n",
    "import pandas as pd\n",
    "import glob\n",
    "import regex as re\n",
    "\n",
    "\n",
    "def safe_get_prefix(attribute, key, prefix_length=3):\n",
    "    value = attribute.get(key)\n",
    "    return value[:prefix_length] if value else None\n",
    "\n",
    "\n",
    "def get_ReportAndDate(filename) -> tuple[str, str]:\n",
    "\n",
    "    #File is formatted as follows:  \"call-report051-2023-12-31-v265-pres.xml\"\n",
    "    #search_pattern = '([^/]+$)'\n",
    "    search_pattern = 'Form[0-9]{3}'\n",
    "    fn_report = re.search(search_pattern,filename).group()\n",
    "    #fn_date_split = filename.replace('call-report','').split('-')\n",
    "    fn_date_split = filename.split('-')\n",
    "    fn_date = f'{fn_date_split[2]}-{fn_date_split[3]}-{fn_date_split[4]}' if (fn_date_split[2] and fn_date_split[3] and fn_date_split[4]) else None\n",
    "    return (fn_report, fn_date)\n",
    "\n",
    "\n",
    "# Print the structure of the presentation file\n",
    "# These taxonomies have a very particular and nuanced structure\n",
    "def parse_pres(input_path= paths.localPath + paths.folder_Taxonomies, input_search_pattern= '*/*-pres.xml', input_df= None, output_filepath= paths.folder_Orig + paths.filename_MDRM):\n",
    "\n",
    "    pres_dtype = {'from':str, 'to':str, 'date':str, 'report':str}\n",
    "    pres_df = pd.DataFrame(columns=['from', 'to', 'date', 'report']) #, dtype=pres_dtype)\n",
    "\n",
    "    for file_path in glob.glob(input_path + input_search_pattern):\n",
    "\n",
    "        file_df = pd.DataFrame(columns=['from', 'to', 'date', 'report']) #, dtype=pres_dtype)\n",
    "\n",
    "        # Parse the presentation linkbase file\n",
    "        xml_tree = ET.parse(file_path)\n",
    "        xml_root = xml_tree.getroot()\n",
    "\n",
    "        for child in xml_root:\n",
    "            #print(child.tag, child.attrib)\n",
    "\n",
    "            if child.tag[child.tag.find('}')+1:] == 'presentationLink':\n",
    "                for i,ch in enumerate(child):\n",
    "\n",
    "                    tag = remove_text(ch.tag)\n",
    "                    attrib = remove_text(ch.attrib)\n",
    "                    label_prefix = safe_get_prefix(attrib, 'label')\n",
    "                    from_prefix = safe_get_prefix(attrib, 'from')\n",
    "                    to_prefix = safe_get_prefix(attrib, 'to')\n",
    "\n",
    "                    if tag == 'presentationArc' and (label_prefix == 'cc_' or to_prefix == 'cc_'):\n",
    "                        #att_label = attrib.get(\"label\", \"N/A\")\n",
    "                        att_from = attrib.get('from', 'N/A')\n",
    "                        att_to = attrib.get(\"to\", \"N/A\")\n",
    "                        if att_from:\n",
    "                            att_report, att_date = get_ReportAndDate(file_path) \n",
    "                            file_data = pd.DataFrame({'from': att_from, 'to': att_to, 'date':att_date, 'report':att_report}, index=[0]) #, dtype=pres_dtype)\n",
    "                            file_df = pd.concat([file_df, file_data])\n",
    "        \n",
    "        pres_df = pd.concat([pres_df, file_df], ignore_index=True)\n",
    "    pres_df.to_csv(output_filepath, sep=',', quotechar='\"', index= False)  \n",
    "\n",
    "\n",
    "\n",
    "def parse_cap(input_path= paths.localPath+paths.folder_Taxonomies, input_search_pattern = '*/*-cap.xml', input_df= None, output_filepath= paths.folder_Orig+paths.filename_MDRM):\n",
    "    cap_dtype = {'from':str, 'to':str, 'label':str, 'text':str}\n",
    "    cap_df = pd.DataFrame(columns=['from', 'to', 'label', 'text']) #, dtype=cap_dtype)\n",
    "\n",
    "    for file_path in glob.glob(input_path + input_search_pattern):\n",
    "\n",
    "        file_df = pd.DataFrame(columns=['from', 'to', 'label', 'text']) #, dtype=cap_dtype)\n",
    "\n",
    "        # Parse the presentation linkbase file\n",
    "        xml_tree = ET.parse(file_path)\n",
    "        xml_root = xml_tree.getroot()\n",
    "\n",
    "        for child in xml_root:\n",
    "            #print(child.tag, child.attrib)\n",
    "\n",
    "            if child.tag[child.tag.find('}')+1:] == 'labelLink':\n",
    "                for i,ch in enumerate(child):\n",
    "\n",
    "                    tag = remove_text(ch.tag)\n",
    "                    attrib = remove_text(ch.attrib)\n",
    "\n",
    "                    if tag == 'labelArc':\n",
    "                        att_from = attrib.get('from', 'N/A')\n",
    "                        att_to = attrib.get('to', 'N/A')\n",
    "                        \n",
    "                    if tag == 'label':\n",
    "                        att_label = attrib.get('label', 'N/A')\n",
    "                        att_text = ch.text\n",
    "                        \n",
    "                        if att_label:\n",
    "                            file_data = pd.DataFrame({'from': att_from, 'to': att_to, 'label':att_label, 'text':att_text}, index=[0]) #, dtype=pres_dtype)\n",
    "                            file_df = pd.concat([file_df, file_data])\n",
    "                            #Reset values of all attribute variables\n",
    "                            att_from, att_to, att_label, att_text = None, None, None, None\n",
    "\n",
    "\n",
    "                    #[2 iterations to fill each of these]\n",
    "                    # check row label = labelArc\n",
    "                        #set values\n",
    "                    # check row label = label\n",
    "                        #set values\n",
    "\n",
    "                    #[Run every 2 iterations or when all vals are full??]\n",
    "                    # if labelArc or label\n",
    "                        #set df values; append to df\n",
    "\n",
    "        cap_df = pd.concat([cap_df, file_df], ignore_index=True)\n",
    "    cap_df.to_csv(output_filepath, sep=',', quotechar='\"', index= False)  \n",
    "\n",
    "\n",
    "\n",
    "def merge_dataframes(dataframes_and_mergeColumns: list[tuple[pd.DataFrame, str]], is_filepath=False, how='inner'):\n",
    "    \"\"\"\n",
    "    Merges multiple dataframes into a single dataframe based on specified columns.\n",
    "\n",
    "    :param dataframes_and_mergeColumns: List of tuples containing either DataFrames or file paths and their corresponding\n",
    "                                merge column names. \n",
    "                                Format: [(df_or_path, merge_column), (df_or_path, merge_column), ...]\n",
    "    :param filepaths: Boolean indicating if the input is a list of file paths. Default is False.\n",
    "    :param how: Type of merge to be performed. Default is 'inner'. Other options: 'outer', 'left', 'right'.\n",
    "    :return: Merged pandas DataFrame.\n",
    "    \"\"\"\n",
    "    if is_filepath:\n",
    "        # Load DataFrames from CSV files\n",
    "        dataframes = [(pd.read_csv(df_or_path), col) for df_or_path, col in dataframes_and_mergeColumns]\n",
    "    else:\n",
    "        dataframes = dataframes_and_mergeColumns\n",
    "\n",
    "    # Initialize merged_df with the first DataFrame and its corresponding merge column\n",
    "    merged_df, left_on = dataframes[0]\n",
    "\n",
    "    for df, right_on in dataframes[1:]:\n",
    "        # Merge on specified columns from each DataFrame\n",
    "        merged_df = pd.merge(merged_df, df, left_on=left_on, right_on=right_on, how=how)\n",
    "        left_on = right_on  # Update left_on for the next merge\n",
    "\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "# Example usage:\n",
    "# If you have a list of file paths and their corresponding columns:\n",
    "# merged_df = merge_dataframes([(\"file1.csv\", \"col1\"), (\"file2.csv\", \"col2\"), (\"file3.csv\", \"col3\")],\n",
    "#                              filepaths=True)\n",
    "\n",
    "# If you have a list of DataFrames and their corresponding columns:\n",
    "# merged_df = merge_dataframes([(df1, \"col1\"), (df2, \"col2\"), (df3, \"col3\")],\n",
    "#                              filepaths=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cap TEST\n",
    "#path = paths.localPath + paths.folder_Taxonomies\n",
    "#file_search_pattern = '*/*-cap.xml'\n",
    "file_search_pattern = ''\n",
    "path = '/Users/kris/data_sources/fdic/Src/Taxonomies/Call_ 12312023_Form051/call-report051-2023-12-31-v265-cap.xml'\n",
    "output = 'FDIC/MDRM_cap_test.csv'\n",
    "\n",
    "parse_cap(input_path=path, input_search_pattern=file_search_pattern, output_filepath=output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pres_file = 'FDIC/MDRM_test.csv'\n",
    "cap_file = 'FDIC/MDRM_cap_test.csv'\n",
    "\n",
    "pres_df = pd.read_csv(pres_file, delimiter=',')\n",
    "cap_file = pd.read_csv(cap_file, delimiter=',')\n",
    "\n",
    "#test = merge_dataframes([pres_df, cap_file], ['from', 'from'])\n",
    "test = merge_dataframes([(pres_df, 'from'), (cap_file, 'from')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from FDIC import constants as paths\n",
    "\n",
    "def build_MDRMdict(input_path =paths.localPath+paths.folder_MDRMs+paths.filename_MDRM_src, export_path =paths.folder_Orig + paths.filename_MDRM, filters=None):\n",
    "    \"\"\"\n",
    "    Reads a CSV file, filters the DataFrame based on the given criteria, \n",
    "    and exports the result to a new CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - input_path: str, path to the input CSV file.\n",
    "    - export_path: str, path where the filtered DataFrame should be saved.\n",
    "    - filters: dict, optional. A dictionary where keys are column names and values \n",
    "               are lists of filter criteria.\n",
    "\n",
    "    Example:\n",
    "    build_MDRMdict(\n",
    "        input_path='path/to/MDRM_CSV.csv',\n",
    "        export_path='path/to/export/MDRM_dict.csv',\n",
    "        filters={'Reporting Form': ['FFIEC 031', 'FFIEC 041', 'FFIEC 051']})\n",
    "    \"\"\"\n",
    "\n",
    "    def format_str_date(date_string):\n",
    "        try:\n",
    "            date_part = date_string.split()[0]\n",
    "            month, day, year = date_part.split('/')\n",
    "            return f\"{year.zfill(4)}-{month.zfill(2)}-{day.zfill(2)}\"\n",
    "            #df[['Formatted Start Date', 'Formatted End Date']] = df[['Start Date', 'End Date']].apply(lambda x: x.str.split().str[0].str.split('/').apply(lambda parts: f\"{parts[2].zfill(4)}-{parts[0].zfill(2)}-{parts[1].zfill(2)}\"))\n",
    "        except:\n",
    "            return date_string\n",
    "\n",
    "    # Read the CSV file with headers starting on line 2\n",
    "    df = pd.read_csv(input_path, header=1, sep=',', quotechar='\"') #, parse_dates=['Start Date', 'End Date'])\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "    df[['Start Date', 'End Date']] = df.apply(lambda row: pd.Series([\n",
    "        format_str_date(row['Start Date']),\n",
    "        format_str_date(row['End Date'])]),\n",
    "        axis=1)\n",
    "    #df[['Start Date', 'End Date']] = df[['Start Date', 'End Date']].apply(lambda x: '-'.join(x.split()[0].split('/')[::-1]).zfill(10))\n",
    "\n",
    "    # Apply filters if provided\n",
    "    if filters:\n",
    "        for column, values in filters.items():\n",
    "            df = df[df[column].isin(values)]\n",
    "    \n",
    "    #Rename columns and format df to needed format and order\n",
    "    rename_columns={'Confidentiality':'Confidential',\n",
    "                    'ItemType':'Item Type',\n",
    "                    'SeriesGlossary':'Series Glossary'}\n",
    "    df.rename(columns=rename_columns, inplace=True)\n",
    "    df['MDRM_Item'] = df['Mnemonic'] + df['Item Code'].astype(str)\n",
    "    df = df[['MDRM_Item', 'Start Date', 'End Date', 'Item Name', 'Confidential', 'Reporting Form']]\n",
    "\n",
    "    # Export the filtered DataFrame to the specified export path\n",
    "    df.to_csv(export_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build_MDRMdict(export_path =paths.folder_Orig + 'MDRM_test.csv' ,filters={'Reporting Form':['FFIEC 031', 'FFIEC 041', 'FFIEC 051']})\n",
    "build_MDRMdict(export_path =paths.folder_Orig + 'MDRM_test.csv' ,filters={'Reporting Form':['FFIEC 031', 'FFIEC 041', 'FFIEC 051']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "data = [{'tic': 'AAPL',\n",
    "  'cusip': 'A00000',\n",
    "  'datadate': Timestamp('1979-12-31 00:00:00'),\n",
    "  'fyearq': 1979,\n",
    "  'fqtr': 4,\n",
    "  'revtq': 50.0},\n",
    " {'tic': 'MSFT',\n",
    "  'cusip': 'A11111',\n",
    "  'datadate': Timestamp('1979-12-31 00:00:00'),\n",
    "  'fyearq': 1979,\n",
    "  'fqtr': 4,\n",
    "  'revtq': 75.0},\n",
    " {'tic': 'AAPL',\n",
    "  'cusip': 'A00000',\n",
    "  'datadate': Timestamp('1980-03-31 00:00:00'),\n",
    "  'fyearq': 1980,\n",
    "  'fqtr': 1,\n",
    "  'revtq': 53.5},\n",
    " {'tic': 'MSFT',\n",
    "  'cusip': 'A11111',\n",
    "  'datadate': Timestamp('1980-03-31 00:00:00'),\n",
    "  'fyearq': 1980,\n",
    "  'fqtr': 1,\n",
    "  'revtq': 78.0},\n",
    " {'tic': 'AAPL',\n",
    "  'cusip': 'A00000',\n",
    "  'datadate': Timestamp('1980-06-30 00:00:00'),\n",
    "  'fyearq': 1980,\n",
    "  'fqtr': 2,\n",
    "  'revtq': 56.71},\n",
    " {'tic': 'MSFT',\n",
    "  'cusip': 'A11111',\n",
    "  'datadate': Timestamp('1980-06-30 00:00:00'),\n",
    "  'fyearq': 1980,\n",
    "  'fqtr': 2,\n",
    "  'revtq': 85.80000000000001},\n",
    " {'tic': 'AAPL',\n",
    "  'cusip': 'A00000',\n",
    "  'datadate': Timestamp('1980-09-30 00:00:00'),\n",
    "  'fyearq': 1980,\n",
    "  'fqtr': 3,\n",
    "  'revtq': 60.679700000000004},\n",
    " {'tic': 'MSFT',\n",
    "  'cusip': 'A11111',\n",
    "  'datadate': Timestamp('1980-09-30 00:00:00'),\n",
    "  'fyearq': 1980,\n",
    "  'fqtr': 3,\n",
    "  'revtq': 94.38000000000002},\n",
    " {'tic': 'AAPL',\n",
    "  'cusip': 'A00000',\n",
    "  'datadate': Timestamp('1980-12-31 00:00:00'),\n",
    "  'fyearq': 1980,\n",
    "  'fqtr': 4,\n",
    "  'revtq': 66.14087300000001},\n",
    " {'tic': 'MSFT',\n",
    "  'cusip': 'A11111',\n",
    "  'datadate': Timestamp('1980-12-31 00:00:00'),\n",
    "  'fyearq': 1980,\n",
    "  'fqtr': 4,\n",
    "  'revtq': 95.32380000000002},\n",
    " {'tic': 'AAPL',\n",
    "  'cusip': 'A00000',\n",
    "  'datadate': Timestamp('1981-03-31 00:00:00'),\n",
    "  'fyearq': 1981,\n",
    "  'fqtr': 1,\n",
    "  'revtq': 70.77073411000002},\n",
    " {'tic': 'MSFT',\n",
    "  'cusip': 'A11111',\n",
    "  'datadate': Timestamp('1981-03-31 00:00:00'),\n",
    "  'fyearq': 1981,\n",
    "  'fqtr': 1,\n",
    "  'revtq': 104.85618000000002},\n",
    " {'tic': 'AAPL',\n",
    "  'cusip': 'A00000',\n",
    "  'datadate': Timestamp('1981-06-30 00:00:00'),\n",
    "  'fyearq': 1981,\n",
    "  'fqtr': 2,\n",
    "  'revtq': 72.89385613330002},\n",
    " {'tic': 'MSFT',\n",
    "  'cusip': 'A11111',\n",
    "  'datadate': Timestamp('1981-06-30 00:00:00'),\n",
    "  'fyearq': 1981,\n",
    "  'fqtr': 2,\n",
    "  'revtq': 110.09898900000003},\n",
    " {'tic': 'AAPL',\n",
    "  'cusip': 'A00000',\n",
    "  'datadate': Timestamp('1981-09-30 00:00:00'),\n",
    "  'fyearq': 1981,\n",
    "  'fqtr': 3,\n",
    "  'revtq': 75.80961037863203},\n",
    " {'tic': 'MSFT',\n",
    "  'cusip': 'A11111',\n",
    "  'datadate': Timestamp('1981-09-30 00:00:00'),\n",
    "  'fyearq': 1981,\n",
    "  'fqtr': 3,\n",
    "  'revtq': 121.10888790000004},\n",
    " {'tic': 'AAPL',\n",
    "  'cusip': 'A00000',\n",
    "  'datadate': Timestamp('1981-12-31 00:00:00'),\n",
    "  'fyearq': 1981,\n",
    "  'fqtr': 4,\n",
    "  'revtq': 77.32580258620467},\n",
    " {'tic': 'MSFT',\n",
    "  'cusip': 'A11111',\n",
    "  'datadate': Timestamp('1981-12-31 00:00:00'),\n",
    "  'fyearq': 1981,\n",
    "  'fqtr': 4,\n",
    "  'revtq': 133.21977669000006},\n",
    " {'tic': 'AAPL',\n",
    "  'cusip': 'A00000',\n",
    "  'datadate': Timestamp('1982-03-31 00:00:00'),\n",
    "  'fyearq': 1982,\n",
    "  'fqtr': 1,\n",
    "  'revtq': 77.32580258620467},\n",
    " {'tic': 'MSFT',\n",
    "  'cusip': 'A11111',\n",
    "  'datadate': Timestamp('1982-03-31 00:00:00'),\n",
    "  'fyearq': 1982,\n",
    "  'fqtr': 1,\n",
    "  'revtq': 133.21977669000006},\n",
    " {'tic': 'AAPL',\n",
    "  'cusip': 'A00000',\n",
    "  'datadate': Timestamp('1982-06-30 00:00:00'),\n",
    "  'fyearq': 1982,\n",
    "  'fqtr': 2,\n",
    "  'revtq': 77.32580258620467},\n",
    " {'tic': 'MSFT',\n",
    "  'cusip': 'A11111',\n",
    "  'datadate': Timestamp('1982-06-30 00:00:00'),\n",
    "  'fyearq': 1982,\n",
    "  'fqtr': 2,\n",
    "  'revtq': 135.88417222380005},\n",
    " {'tic': 'AAPL',\n",
    "  'cusip': 'A00000',\n",
    "  'datadate': Timestamp('1982-09-30 00:00:00'),\n",
    "  'fyearq': 1982,\n",
    "  'fqtr': 3,\n",
    "  'revtq': 85.05838284482515},\n",
    " {'tic': 'MSFT',\n",
    "  'cusip': 'A11111',\n",
    "  'datadate': Timestamp('1982-09-30 00:00:00'),\n",
    "  'fyearq': 1982,\n",
    "  'fqtr': 3,\n",
    "  'revtq': 144.03722255722806},\n",
    " {'tic': 'AAPL',\n",
    "  'cusip': 'A00000',\n",
    "  'datadate': Timestamp('1982-12-31 00:00:00'),\n",
    "  'fyearq': 1982,\n",
    "  'fqtr': 4,\n",
    "  'revtq': 97.4033828448251},\n",
    " {'tic': 'MSFT',\n",
    "  'cusip': 'A11111',\n",
    "  'datadate': Timestamp('1982-12-31 00:00:00'),\n",
    "  'fyearq': 1982,\n",
    "  'fqtr': 4,\n",
    "  'revtq': 178.982222557228}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time.struct_time(tm_year=2024, tm_mon=8, tm_mday=20, tm_hour=16, tm_min=16, tm_sec=26, tm_wday=1, tm_yday=233, tm_isdst=1)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "print(time.localtime(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
